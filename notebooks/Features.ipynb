{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so we can use packages from parent directory\n",
    "import sys\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from collections import Counter\n",
    "import torch\n",
    "import numpy as np\n",
    "import skimage.color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'monroe_data' from '/Users/benjaminnewman/Documents/Stanford/Freshman_2017-2018/WINTER/LINGUIST130A/linguist-130a-final-proj/monroe_data.py'>"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(monroe_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def tokenize(self, sentence):\n",
    "        pass\n",
    "\n",
    "class WhitespaceTokenizer(Tokenizer):\n",
    "    def tokenize(self, sentence):\n",
    "        return nltk.word_tokenize(sentence)\n",
    "    \n",
    "class EndingTokenizer(Tokenizer):\n",
    "    \"\"\"\n",
    "    Segments endings as different words from words that end with them:\n",
    "    Ex: 'greener' -> 'green', 'er'\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # Endings defined here:\n",
    "        # https://github.com/futurulus/colors-in-context/blob/2e7b830668cd039830154e7e8f211c6d4415d30f/tokenizers.py#L35\n",
    "        self.endings = ['er', 'est', 'ish']\n",
    "        \n",
    "    def tokenize(self, sentence):\n",
    "        tokens = []\n",
    "        for word in nltk.word_tokenize(sentence):\n",
    "            inserted = False\n",
    "            for ending in self.endings:\n",
    "                if word.endswith(ending):\n",
    "                    tokens.extend([word[:-len(ending)], '+{}'.format(ending)])\n",
    "                    inserted = True\n",
    "                    break\n",
    "            if not inserted:\n",
    "                tokens.append(word)\n",
    "        return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CaptionIndexer:\n",
    "    def __init__(self):\n",
    "        self.UNK = '<unk>'\n",
    "        self.EOS = '<eos>'\n",
    "        self.SOS = '<sos>'\n",
    "        \n",
    "        \n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.word_count = Counter()\n",
    "        self.size = 0\n",
    "        \n",
    "        \n",
    "    def add_sentence(self, sentence):\n",
    "        for word in sentence:\n",
    "            word = word.lower()\n",
    "            if not word in self.word2idx:\n",
    "                self.word2idx[word] = self.size\n",
    "                self.idx2word[self.size] = word\n",
    "                self.size += 1\n",
    "            self.word_count[word] += 1\n",
    "        \n",
    "    def get_word_from_idx(self, idx):\n",
    "        return self.idx2word[idx]\n",
    "    \n",
    "    def get_idx_from_word(self, word):\n",
    "        return self.word2idx.get(word, self.word2idx[self.UNK])\n",
    "    \n",
    "    def to_indices(self, sentence, construct=False):\n",
    "        if construct:\n",
    "            self.add_sentence(sentence)\n",
    "            # we know everything is in the map because we just added it\n",
    "            return [self.word2idx[word] for word in sentence]\n",
    "        \n",
    "        return [self.get_idx_from_word(word) for word in sentence]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CaptionFeaturizer:\n",
    "    \n",
    "    def __init__(self, tokenizer=WhitespaceTokenizer, unk_threshold=1):\n",
    "        self.tokenizer = tokenizer()\n",
    "        self.caption_indexer = CaptionIndexer()\n",
    "        self.word_count = None\n",
    "        \n",
    "        # hyperparams\n",
    "        self.unk_threshold = unk_threshold\n",
    "    \n",
    "    def to_tensor(self, caption, construct=False):\n",
    "        _, indexes = self.to_string_features(caption, construct)\n",
    "        return torch.tensor(indexes, dtype=torch.long).view(-1, 1)\n",
    "    \n",
    "    def to_string_features(self, caption, construct=False):\n",
    "        \"\"\"\n",
    "        Params:\n",
    "        caption:   string hodling caption that will converted to tokens and\n",
    "                   indices. \n",
    "        construct: if we are constructing the featurizer for the first time,\n",
    "                   this should be true. It performs the unk substitutions \n",
    "                   manually based on the contents of self.word_count and \n",
    "                   also adds the sentences to the indexer. Should only be\n",
    "                   true when training for the first time.\n",
    "                   \n",
    "        Returns:\n",
    "        Tuple(tokens, indices). \n",
    "        \n",
    "        tokens is a tokenized version the passed caption,\n",
    "                unks are replaced, words are lower cased, buffered on both sides by sos/\n",
    "                eos tags\n",
    "        indices is a list indices given by the indexer for each token. These can be converted\n",
    "                to tensor to be fed into the model\n",
    "        \n",
    "        \"\"\"\n",
    "        caption_tokens = self.tokenizer.tokenize(caption)\n",
    "        caption_tokens = self.to_model_format(caption_tokens, construct)\n",
    "        caption_indices = self.caption_indexer.to_indices(caption_tokens, construct)\n",
    "        caption_tokens = [self.caption_indexer.get_word_from_idx(index) for index in caption_indices]\n",
    "        return caption_tokens, caption_indices\n",
    "        \n",
    "    def to_model_format(self, tokens, construct):\n",
    "        \"\"\"\n",
    "        Put the tokens into the format expected by the models.\n",
    "        This mainly entails prepending/appending <sos>, <eos>,\n",
    "        lowercasing all of the words and replacing all uncommon words\n",
    "        with <unk> (only in the case when we are constructing the\n",
    "        featurizer for the first time)\n",
    "        \n",
    "        Params:\n",
    "        tokens: \n",
    "        construct: if we are constructing the featurizer for the first time,\n",
    "                   this should be true. It performs the unk substitutions \n",
    "                   manually based on the contents of self.word_count and \n",
    "                   also adds the sentences to the indexer. Should only be\n",
    "                   true when training for the first time.\n",
    "        \"\"\"\n",
    "        if construct:\n",
    "            if self.word_count is None:\n",
    "                print(\"FEATURIZER HAS NOT BEEN CONSTRUCTED YET. Call `construct_featurizer`\")\n",
    "            else:\n",
    "                for i in range(len(tokens)):\n",
    "                    if self.word_count[tokens[i]] <= self.unk_threshold:\n",
    "                        tokens[i] = self.caption_indexer.UNK\n",
    "                        \n",
    "        tokens = [token.lower() for token in tokens]\n",
    "        tokens = [self.caption_indexer.SOS] + tokens + [self.caption_indexer.EOS]\n",
    "        return tokens\n",
    "    \n",
    "    def construct_featurizer(self, data_entries):\n",
    "        \"\"\"\n",
    "        data_entries is of type MonroeData. \n",
    "        \"\"\"\n",
    "        self.word_count = Counter()\n",
    "        for entry in data_entries:\n",
    "            caption_tokens = self.tokenizer.tokenize(entry.caption)\n",
    "            for token in caption_tokens:\n",
    "                self.word_count[token] += 1\n",
    "                \n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "from monroe_data import MonroeData, MonroeDataEntry, Color # last two for reading pkl file\n",
    "#import Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "monroe_data_train = monroe_data.MonroeData(\"train_corpus_monroe.csv\", single_speaker=True, ss_method=\"pool\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 8s, sys: 373 ms, total: 1min 8s\n",
      "Wall time: 1min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for _ in monroe_data_train.read_data():\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[hsl: [226, 81, 50], rgb [24, 73, 232], hsv [226, 89.50276243093923, 90.5],\n",
       " hsl: [283, 87, 50], rgb [176, 17, 239], hsv [283, 93.04812834224599, 93.5],\n",
       " hsl: [248, 92, 50], rgb [42, 10, 246], hsv [248, 95.83333333333333, 96.0]]"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monroe_data_train[0].colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "monroe_data_train.save_entries(\"train_entries_monroe.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "monroe_data_dev = monroe_data.MonroeData(\"dev_corpus_monroe.csv\", single_speaker=True, ss_method=\"pool\")\n",
    "for _ in monroe_data_dev.read_data():\n",
    "    pass\n",
    "monroe_data_dev.save_entries(\"dev_entries_monroe.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monroe_data_train[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurizer = CaptionFeaturizer(EndingTokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurizer.construct_featurizer(monroe_data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = []\n",
    "captions = []\n",
    "\n",
    "for entry in monroe_data_train:\n",
    "    i, c = featurizer.to_string_features(entry.caption, construct=True)\n",
    "    indices.append(i)\n",
    "    captions.append(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['<sos>', 'the', 'dark', '+er', 'blue', 'one', '<eos>'],\n",
       " ['<sos>', 'purple', '<eos>'],\n",
       " ['<sos>', 'medium', 'pink', 'the', 'medium', 'dark', 'one', '<eos>'],\n",
       " ['<sos>', 'lime', '<eos>']]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 6, 2]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "captions[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['<sos>', 'the', 'blu', '+est', 'blue', 'of', 'the', '<unk>', '<eos>'],\n",
       " [0, 1, 100, 20, 4, 22, 1, 11, 6])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featurizer.to_string_features(\"the bluest blue of the posedien\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0],\n",
       "        [  1],\n",
       "        [100],\n",
       "        [ 20],\n",
       "        [  4],\n",
       "        [ 22],\n",
       "        [  1],\n",
       "        [ 11],\n",
       "        [  6]])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featurizer.to_tensor(\"the bluest blue of the posedien\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def color_phi_id(color_list, space):\n",
    "    \"\"\"\n",
    "    Function for turning a list of colors in the given space\n",
    "    (with normalization marked by \"_norm\") into a feature function.\n",
    "    \n",
    "    This is just the identity feature function, so it's kind of boring,\n",
    "    but we can imagine doing more complext things too (like fourier\n",
    "    transform). We pass space as well in case you want to do different\n",
    "    operations based on the space or only have a feature function work\n",
    "    for HSL for example\n",
    "    \n",
    "    ex:\n",
    "    color_list = [256, 0, 0] space = 'rgb'\n",
    "    color_list = [1, 0, 0] space = 'rgb_norm'\n",
    "    \"\"\"\n",
    "    return color_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "def color_phi_fourier(color_list, space, resolution=3):\n",
    "    if space != \"rgb_norm\":\n",
    "        print(\"Space must be rgb_norm to use fourier transform\")\n",
    "        return None\n",
    "\n",
    "    resolution = [resolution for _ in color_list]\n",
    "    colors = np.array([color_list])\n",
    "    ranges = np.array([256, 256, 256])\n",
    "    \n",
    "    xyz = colors / 2\n",
    "\n",
    "    ax, ay, az = [np.arange(0, g) for g, r in zip(resolution, ranges)]\n",
    "    gx, gy, gz = np.meshgrid(ax, ay, az)\n",
    "\n",
    "    arg = (np.multiply.outer(xyz[:, 0], gx) +\n",
    "           np.multiply.outer(xyz[:, 1], gy) +\n",
    "           np.multiply.outer(xyz[:, 2], gz))\n",
    "    #assert arg.shape == (xyz.shape[0],) + tuple(self.resolution), arg.shape\n",
    "    repr_complex = np.exp(-2j * np.pi * (arg % 1.0)).swapaxes(1, 2).reshape((xyz.shape[0], -1))\n",
    "    result = np.hstack([repr_complex.real, repr_complex.imag]).astype(np.float32)\n",
    "    return result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(color_phi_fourier([1., 0., 0.], 'rgb_norm', resolution=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANGES_RGB = (256.0, 256.0, 256.0)\n",
    "RANGES_HSV = (361.0, 101.0, 101.0)\n",
    "C_EPSILON = 1e-4\n",
    "Shsv = True\n",
    "Sresolution = [3, 3, 3]\n",
    "def vectorize_all(colors, hsv=None, Shsv = True):\n",
    "    '''\n",
    "    >>> normalize = lambda v: np.where(v.round(2) == 0.0, 0.0, v.round(2))\n",
    "    >>> normalize(FourierVectorizer([2]).vectorize_all([(255, 0, 0), (0, 255, 255)]))\n",
    "    array([[ 1.,  1.,  1.,  1., -1., -1., -1., -1.,  0.,  0.,  0.,  0.,  0.,\n",
    "             0.,  0.,  0.],\n",
    "           [ 1., -1., -1.,  1.,  1., -1., -1.,  1.,  0.,  0.,  0.,  0.,  0.,\n",
    "             0.,  0.,  0.]], dtype=float32)\n",
    "    '''\n",
    "    if hsv is None:\n",
    "        hsv = Shsv\n",
    "\n",
    "    colors = np.array([colors])\n",
    "    assert len(colors.shape) == 3, colors.shape\n",
    "    assert colors.shape[2] == 3, colors.shape\n",
    "\n",
    "    ranges = np.array(RANGES_HSV if Shsv else RANGES_RGB)\n",
    "    print(\"ranges:\", ranges)\n",
    "    if hsv and not Shsv:\n",
    "        print(\"Converting hsv to rgb\")\n",
    "        c_hsv = colors\n",
    "        color_0_1 = skimage.color.hsv2rgb(c_hsv / (np.array(RANGES_HSV) - 1.0))\n",
    "    elif not hsv and Shsv:\n",
    "        print(\"Converting rgb to hsv\")\n",
    "        c_rgb = colors\n",
    "        color_0_1 = skimage.color.rgb2hsv(c_rgb / (np.array(RANGES_RGB) - 1.0))\n",
    "    else:\n",
    "        print(\"Just normalize\")\n",
    "        color_0_1 = colors / (ranges - 1.0)\n",
    "\n",
    "    print(\"Color (0-1):\", color_0_1)\n",
    "    # Using a Fourier representation causes colors at the boundary of the\n",
    "    # space to behave as if the space is toroidal: red = 255 would be\n",
    "    # about the same as red = 0. We don't want this...\n",
    "    xyz = color_0_1[0] / 2.0\n",
    "    if Shsv:\n",
    "        # ...*except* in the case of HSV: H is in fact a polar coordinate.\n",
    "        xyz[:, 0] *= 2.0\n",
    "\n",
    "    # ax, ay, az = [np.hstack([np.arange(0, g / 2), np.arange(r - g / 2, r)])\n",
    "    #               for g, r in zip(self.resolution, ranges)]\n",
    "    ax, ay, az = [np.arange(0, g) for g, r in zip(Sresolution, ranges)]\n",
    "    gx, gy, gz = np.meshgrid(ax, ay, az)\n",
    "\n",
    "    arg = (np.multiply.outer(xyz[:, 0], gx) +\n",
    "           np.multiply.outer(xyz[:, 1], gy) +\n",
    "           np.multiply.outer(xyz[:, 2], gz))\n",
    "    assert arg.shape == (xyz.shape[0],) + tuple(Sresolution), arg.shape\n",
    "    repr_complex = np.exp(-2j * np.pi * (arg % 1.0)).swapaxes(1, 2).reshape((xyz.shape[0], -1))\n",
    "    result = np.hstack([repr_complex.real, repr_complex.imag]).astype(np.float32)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0., 1., 1.]]])"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skimage.color.rgb2hsv([[(1., 0., 0.)]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colors: [[1.   0.5  0.25]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 1.  ,  0.71,  0.  ,  0.  , -0.71, -1.  , -1.  , -0.71,  0.  ,\n",
       "        -1.  , -0.71,  0.  ,  0.  ,  0.71,  1.  ,  1.  ,  0.71,  0.  ,\n",
       "         1.  ,  0.71,  0.  ,  0.  , -0.71, -1.  , -1.  , -0.71,  0.  ,\n",
       "         0.  , -0.71, -1.  , -1.  , -0.71,  0.  ,  0.  ,  0.71,  1.  ,\n",
       "         0.  ,  0.71,  1.  ,  1.  ,  0.71,  0.  ,  0.  , -0.71, -1.  ,\n",
       "         0.  , -0.71, -1.  , -1.  , -0.71,  0.  ,  0.  ,  0.71,  1.  ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize(color_phi_fourier([1., 0.5, 0.25], 'rgb_norm', resolution=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ranges: [256. 256. 256.]\n",
      "Converting hsv to rgb\n",
      "Color (0-1): [[[1.   0.5  0.25]]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 1.  ,  0.71,  0.  ,  0.  , -0.71, -1.  , -1.  , -0.71,  0.  ,\n",
       "        -1.  , -0.71,  0.  ,  0.  ,  0.71,  1.  ,  1.  ,  0.71,  0.  ,\n",
       "         1.  ,  0.71,  0.  ,  0.  , -0.71, -1.  , -1.  , -0.71,  0.  ,\n",
       "         0.  , -0.71, -1.  , -1.  , -0.71,  0.  ,  0.  ,  0.71,  1.  ,\n",
       "         0.  ,  0.71,  1.  ,  1.  ,  0.71,  0.  ,  0.  , -0.71, -1.  ,\n",
       "         0.  , -0.71, -1.  , -1.  , -0.71,  0.  ,  0.  ,  0.71,  1.  ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize(vectorize_all([(20, 75, 100)], hsv=True, Shsv=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ranges: [361. 101. 101.]\n",
      "Color (0-1): [[[0. 1. 1.]]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 1., -1.,  1., -1.,  1., -1.,  1., -1.,  1.,  1., -1.,  1., -1.,\n",
       "         1., -1.,  1., -1.,  1.,  1., -1.,  1., -1.,  1., -1.,  1., -1.,\n",
       "         1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.]], dtype=float32)"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize(vectorize_all([(0, 100., 100.)], hsv=True, Shsv=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = lambda v: np.where(v.round(2) == 0.0, 0.0, v.round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1., -1., -1., -1., -1.,\n",
       "        -1., -1., -1., -1., -1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.]], dtype=float32)"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize(vectorize_all([(255, 0, 0)], hsv=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 2., 4.])"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([1.0001, 2, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 4])"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5, 1. ])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[1:]/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColorFeaturizer:\n",
    "    def __init__(self, featurizer, space, **kwargs):\n",
    "        self.featurizer = featurizer\n",
    "        self.space = space\n",
    "        self.featurizer_kwargs = kwargs\n",
    "    \n",
    "    \n",
    "    def to_color_lists(self, colors, normalized):\n",
    "        # non-standard, but use the space as the variable name\n",
    "        # to access the color attribute directly\n",
    "        class_var_name = self.space\n",
    "        if normalized:\n",
    "            class_var_name = \"{}_norm\".format(class_var_name)\n",
    "        return [color.__dict__[class_var_name] for color in colors], class_var_name\n",
    "    \n",
    "    def to_tensor(self, colors, normalized=True):\n",
    "        \"\"\"\n",
    "        Convert colors to tensors where the vectors are the given by applying\n",
    "        the feature function self.featurizer to the colors \n",
    "\n",
    "        returns all colors as |colors| x |phi| matrix\n",
    "        \"\"\"\n",
    "        color_lists, space = self.to_color_lists(colors, normalized) \n",
    "        color_lists = [self.featurizer(color_list, space) for color_list in color_lists]\n",
    "        target = color_lists[0] # target is always first color\n",
    "        color_tensor = torch.tensor(color_lists, dtype=torch.float) # to get column vectors\n",
    "        return color_tensor\n",
    "    \n",
    "    def shuffle_colors(self, color_tensor):\n",
    "        \"\"\"\n",
    "        Randomly permute colors. Keep track of where the the target ends up\n",
    "        for training and error analysis\n",
    "        \"\"\"\n",
    "        permutation = torch.randperm(color_tensor.size(0))\n",
    "        target = torch.argmin(permutation).view(-1) # target always started at 0\n",
    "        return color_tensor[permutation], target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_featurizer = ColorFeaturizer(color_phi_id, \"rgb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_color_tensor = color_featurizer.to_tensor(monroe_data_train.entries[0].colors, normalized = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[hsl: [226, 81, 50], rgb [24, 73, 232],\n",
       " hsl: [283, 87, 50], rgb [176, 17, 239],\n",
       " hsl: [248, 92, 50], rgb [42, 10, 246]]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monroe_data_train.entries[0].colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 24.,  73., 232.],\n",
       "        [176.,  17., 239.],\n",
       "        [ 42.,  10., 246.]])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_color_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[176.,  17., 239.],\n",
       "         [ 42.,  10., 246.],\n",
       "         [ 24.,  73., 232.]]), tensor([2]))"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "color_featurizer.shuffle_colors(test_color_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_featurizer_fourier = ColorFeaturizer(color_phi_fourier, \"rgb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_color_tensor = color_featurizer_fourier.to_tensor(monroe_data_train.entries[0].colors, normalized = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0000e+00, -9.5694e-01,  8.3147e-01,  6.2486e-01, -8.2459e-01,\n",
       "          9.5331e-01, -2.1910e-01, -7.3565e-02,  3.5990e-01,  9.5694e-01,\n",
       "         -1.0000e+00,  9.5694e-01,  3.7132e-01, -6.2486e-01,  8.2459e-01,\n",
       "         -4.9290e-01,  2.1910e-01,  7.3565e-02,  8.3147e-01, -9.5694e-01,\n",
       "          1.0000e+00,  8.5797e-02, -3.7132e-01,  6.2486e-01, -7.2425e-01,\n",
       "          4.9290e-01, -2.1910e-01,  0.0000e+00, -2.9028e-01,  5.5557e-01,\n",
       "         -7.8074e-01,  5.6573e-01, -3.0201e-01, -9.7570e-01,  9.9729e-01,\n",
       "         -9.3299e-01, -2.9028e-01, -1.2246e-16,  2.9028e-01, -9.2851e-01,\n",
       "          7.8074e-01, -5.6573e-01, -8.7009e-01,  9.7570e-01, -9.9729e-01,\n",
       "         -5.5557e-01,  2.9028e-01,  0.0000e+00, -9.9631e-01,  9.2851e-01,\n",
       "         -7.8074e-01, -6.8954e-01,  8.7009e-01, -9.7570e-01],\n",
       "        [ 1.0000e+00, -9.7832e-01,  9.1421e-01,  9.7832e-01, -1.0000e+00,\n",
       "          9.7832e-01,  9.1421e-01, -9.7832e-01,  1.0000e+00, -5.5557e-01,\n",
       "          3.7132e-01, -1.7096e-01, -7.1573e-01,  5.5557e-01, -3.7132e-01,\n",
       "         -8.4485e-01,  7.1573e-01, -5.5557e-01, -3.8268e-01,  5.6573e-01,\n",
       "         -7.2425e-01, -1.8304e-01,  3.8268e-01, -5.6573e-01,  2.4541e-02,\n",
       "          1.8304e-01, -3.8268e-01,  0.0000e+00, -2.0711e-01,  4.0524e-01,\n",
       "         -2.0711e-01, -1.2246e-16,  2.0711e-01, -4.0524e-01,  2.0711e-01,\n",
       "          0.0000e+00, -8.3147e-01,  9.2851e-01, -9.8528e-01, -6.9838e-01,\n",
       "          8.3147e-01, -9.2851e-01, -5.3500e-01,  6.9838e-01, -8.3147e-01,\n",
       "          9.2388e-01, -8.2459e-01,  6.8954e-01,  9.8311e-01, -9.2388e-01,\n",
       "          8.2459e-01,  9.9970e-01, -9.8311e-01,  9.2388e-01],\n",
       "        [ 1.0000e+00, -9.9248e-01,  9.7003e-01,  9.9248e-01, -1.0000e+00,\n",
       "          9.9248e-01,  9.7003e-01, -9.9248e-01,  1.0000e+00,  8.7009e-01,\n",
       "         -9.2388e-01,  9.6378e-01,  8.0321e-01, -8.7009e-01,  9.2388e-01,\n",
       "          7.2425e-01, -8.0321e-01,  8.7009e-01,  5.1410e-01, -6.1523e-01,\n",
       "          7.0711e-01,  4.0524e-01, -5.1410e-01,  6.1523e-01,  2.9028e-01,\n",
       "         -4.0524e-01,  5.1410e-01,  0.0000e+00, -1.2241e-01,  2.4298e-01,\n",
       "         -1.2241e-01, -1.2246e-16,  1.2241e-01, -2.4298e-01,  1.2241e-01,\n",
       "          0.0000e+00, -4.9290e-01,  3.8268e-01, -2.6671e-01, -5.9570e-01,\n",
       "          4.9290e-01, -3.8268e-01, -6.8954e-01,  5.9570e-01, -4.9290e-01,\n",
       "         -8.5773e-01,  7.8835e-01, -7.0711e-01, -9.1421e-01,  8.5773e-01,\n",
       "         -7.8835e-01, -9.5694e-01,  9.1421e-01, -8.5773e-01]])"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_color_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.  , -0.96,  0.83,  0.62, -0.82,  0.95, -0.22, -0.07,  0.36,\n",
       "        0.96, -1.  ,  0.96,  0.37, -0.62,  0.82, -0.49,  0.22,  0.07,\n",
       "        0.83, -0.96,  1.  ,  0.09, -0.37,  0.62, -0.72,  0.49, -0.22,\n",
       "        0.  , -0.29,  0.56, -0.78,  0.57, -0.3 , -0.98,  1.  , -0.93,\n",
       "       -0.29,  0.  ,  0.29, -0.93,  0.78, -0.57, -0.87,  0.98, -1.  ,\n",
       "       -0.56,  0.29,  0.  , -1.  ,  0.93, -0.78, -0.69,  0.87, -0.98],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize(test_color_tensor[0].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[226, 81, 50]"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monroe_data_train.entries[0].colors[0].hsl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.0000000e+00, -9.5694035e-01,  8.3146960e-01,  6.2485951e-01,\n",
       "       -8.2458931e-01,  9.5330602e-01, -2.1910124e-01, -7.3564567e-02,\n",
       "        3.5989505e-01,  9.5694035e-01, -1.0000000e+00,  9.5694035e-01,\n",
       "        3.7131721e-01, -6.2485951e-01,  8.2458931e-01, -4.9289820e-01,\n",
       "        2.1910124e-01,  7.3564567e-02,  8.3146960e-01, -9.5694035e-01,\n",
       "        1.0000000e+00,  8.5797310e-02, -3.7131721e-01,  6.2485951e-01,\n",
       "       -7.2424710e-01,  4.9289820e-01, -2.1910124e-01,  0.0000000e+00,\n",
       "       -2.9028466e-01,  5.5557024e-01, -7.8073722e-01,  5.6573182e-01,\n",
       "       -3.0200595e-01, -9.7570211e-01,  9.9729043e-01, -9.3299282e-01,\n",
       "       -2.9028466e-01, -1.2246469e-16,  2.9028466e-01, -9.2850608e-01,\n",
       "        7.8073722e-01, -5.6573182e-01, -8.7008697e-01,  9.7570211e-01,\n",
       "       -9.9729043e-01, -5.5557024e-01,  2.9028466e-01,  0.0000000e+00,\n",
       "       -9.9631262e-01,  9.2850608e-01, -7.8073722e-01, -6.8954057e-01,\n",
       "        8.7008697e-01, -9.7570211e-01], dtype=float32)"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_color_tensor[0].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
