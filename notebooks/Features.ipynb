{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so we can use packages from parent directory\n",
    "import sys\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from collections import Counter\n",
    "import torch\n",
    "import numpy as np\n",
    "import skimage.color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from monroe_data import MonroeData, MonroeDataEntry, Color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'monroe_data' from '/Users/benjaminnewman/Documents/Stanford/Freshman_2017-2018/WINTER/LINGUIST130A/linguist-130a-final-proj/monroe_data.py'>"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(monroe_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def tokenize(self, sentence):\n",
    "        pass\n",
    "\n",
    "class WhitespaceTokenizer(Tokenizer):\n",
    "    def tokenize(self, sentence):\n",
    "        return nltk.word_tokenize(sentence)\n",
    "    \n",
    "class EndingTokenizer(Tokenizer):\n",
    "    \"\"\"\n",
    "    Segments endings as different words from words that end with them:\n",
    "    Ex: 'greener' -> 'green', 'er'\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # Endings defined here:\n",
    "        # https://github.com/futurulus/colors-in-context/blob/2e7b830668cd039830154e7e8f211c6d4415d30f/tokenizers.py#L35\n",
    "        self.endings = ['er', 'est', 'ish']\n",
    "        \n",
    "    def tokenize(self, sentence):\n",
    "        tokens = []\n",
    "        for word in nltk.word_tokenize(sentence):\n",
    "            inserted = False\n",
    "            for ending in self.endings:\n",
    "                if word.endswith(ending):\n",
    "                    tokens.extend([word[:-len(ending)], '+{}'.format(ending)])\n",
    "                    inserted = True\n",
    "                    break\n",
    "            if not inserted:\n",
    "                tokens.append(word)\n",
    "        return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CaptionIndexer:\n",
    "    def __init__(self):\n",
    "        self.UNK = '<unk>'\n",
    "        self.EOS = '<eos>'\n",
    "        self.SOS = '<sos>'\n",
    "        \n",
    "        \n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.word_count = Counter()\n",
    "        self.size = 0\n",
    "        \n",
    "        \n",
    "    def add_sentence(self, sentence):\n",
    "        for word in sentence:\n",
    "            word = word.lower()\n",
    "            if not word in self.word2idx:\n",
    "                self.word2idx[word] = self.size\n",
    "                self.idx2word[self.size] = word\n",
    "                self.size += 1\n",
    "            self.word_count[word] += 1\n",
    "        \n",
    "    def get_word_from_idx(self, idx):\n",
    "        return self.idx2word[idx]\n",
    "    \n",
    "    def get_idx_from_word(self, word):\n",
    "        return self.word2idx.get(word, self.word2idx[self.UNK])\n",
    "    \n",
    "    def to_indices(self, sentence, construct=False):\n",
    "        if construct:\n",
    "            self.add_sentence(sentence)\n",
    "            # we know everything is in the map because we just added it\n",
    "            return [self.word2idx[word] for word in sentence]\n",
    "        \n",
    "        return [self.get_idx_from_word(word) for word in sentence]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CaptionFeaturizer:\n",
    "    \n",
    "    def __init__(self, tokenizer=WhitespaceTokenizer, unk_threshold=1):\n",
    "        self.tokenizer = tokenizer()\n",
    "        self.caption_indexer = CaptionIndexer()\n",
    "        self.word_count = None\n",
    "        \n",
    "        # hyperparams\n",
    "        self.unk_threshold = unk_threshold\n",
    "    \n",
    "    def to_tensor(self, caption, construct=False):\n",
    "        _, indexes = self.to_string_features(caption, construct)\n",
    "        return torch.tensor(indexes, dtype=torch.long).view(-1, 1)\n",
    "    \n",
    "    def to_string_features(self, caption, construct=False):\n",
    "        \"\"\"\n",
    "        Params:\n",
    "        caption:   string hodling caption that will converted to tokens and\n",
    "                   indices. \n",
    "        construct: if we are constructing the featurizer for the first time,\n",
    "                   this should be true. It performs the unk substitutions \n",
    "                   manually based on the contents of self.word_count and \n",
    "                   also adds the sentences to the indexer. Should only be\n",
    "                   true when training for the first time.\n",
    "                   \n",
    "        Returns:\n",
    "        Tuple(tokens, indices). \n",
    "        \n",
    "        tokens is a tokenized version the passed caption,\n",
    "                unks are replaced, words are lower cased, buffered on both sides by sos/\n",
    "                eos tags\n",
    "        indices is a list indices given by the indexer for each token. These can be converted\n",
    "                to tensor to be fed into the model\n",
    "        \n",
    "        \"\"\"\n",
    "        caption_tokens = self.tokenizer.tokenize(caption)\n",
    "        caption_tokens = self.to_model_format(caption_tokens, construct)\n",
    "        caption_indices = self.caption_indexer.to_indices(caption_tokens, construct)\n",
    "        caption_tokens = [self.caption_indexer.get_word_from_idx(index) for index in caption_indices]\n",
    "        return caption_tokens, caption_indices\n",
    "        \n",
    "    def to_model_format(self, tokens, construct):\n",
    "        \"\"\"\n",
    "        Put the tokens into the format expected by the models.\n",
    "        This mainly entails prepending/appending <sos>, <eos>,\n",
    "        lowercasing all of the words and replacing all uncommon words\n",
    "        with <unk> (only in the case when we are constructing the\n",
    "        featurizer for the first time)\n",
    "        \n",
    "        Params:\n",
    "        tokens: \n",
    "        construct: if we are constructing the featurizer for the first time,\n",
    "                   this should be true. It performs the unk substitutions \n",
    "                   manually based on the contents of self.word_count and \n",
    "                   also adds the sentences to the indexer. Should only be\n",
    "                   true when training for the first time.\n",
    "        \"\"\"\n",
    "        if construct:\n",
    "            if self.word_count is None:\n",
    "                print(\"FEATURIZER HAS NOT BEEN CONSTRUCTED YET. Call `construct_featurizer`\")\n",
    "            else:\n",
    "                for i in range(len(tokens)):\n",
    "                    if self.word_count[tokens[i]] <= self.unk_threshold:\n",
    "                        tokens[i] = self.caption_indexer.UNK\n",
    "                        \n",
    "        tokens = [token.lower() for token in tokens]\n",
    "        tokens = [self.caption_indexer.SOS] + tokens + [self.caption_indexer.EOS]\n",
    "        return tokens\n",
    "    \n",
    "    def construct_featurizer(self, data_entries):\n",
    "        \"\"\"\n",
    "        data_entries is of type MonroeData. \n",
    "        \"\"\"\n",
    "        self.word_count = Counter()\n",
    "        for entry in data_entries:\n",
    "            caption_tokens = self.tokenizer.tokenize(entry.caption)\n",
    "            for token in caption_tokens:\n",
    "                self.word_count[token] += 1\n",
    "                \n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "from monroe_data import MonroeData, MonroeDataEntry, Color # last two for reading pkl file\n",
    "#import Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monroe_data_train = monroe_data.MonroeData(\"train_corpus_monroe.csv\", \"train_entries_monroe.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "monroe_data_train = monroe_data.MonroeData(\"train_corpus_monroe.csv\", single_speaker=True, ss_method=\"pool\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 8s, sys: 373 ms, total: 1min 8s\n",
      "Wall time: 1min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for _ in monroe_data_train.read_data():\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[hsl: [226, 81, 50], rgb [24, 73, 232], hsv [226, 89.50276243093923, 90.5],\n",
       " hsl: [283, 87, 50], rgb [176, 17, 239], hsv [283, 93.04812834224599, 93.5],\n",
       " hsl: [248, 92, 50], rgb [42, 10, 246], hsv [248, 95.83333333333333, 96.0]]"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monroe_data_train[0].colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "monroe_data_train.save_entries(\"train_entries_monroe.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "monroe_data_dev = monroe_data.MonroeData(\"dev_corpus_monroe.csv\", single_speaker=True, ss_method=\"pool\")\n",
    "for _ in monroe_data_dev.read_data():\n",
    "    pass\n",
    "monroe_data_dev.save_entries(\"dev_entries_monroe.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monroe_data_train[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurizer = CaptionFeaturizer(EndingTokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurizer.construct_featurizer(monroe_data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = []\n",
    "captions = []\n",
    "\n",
    "for entry in monroe_data_train:\n",
    "    i, c = featurizer.to_string_features(entry.caption, construct=True)\n",
    "    indices.append(i)\n",
    "    captions.append(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['<sos>', 'the', 'dark', '+er', 'blue', 'one', '<eos>'],\n",
       " ['<sos>', 'purple', '<eos>'],\n",
       " ['<sos>', 'medium', 'pink', 'the', 'medium', 'dark', 'one', '<eos>'],\n",
       " ['<sos>', 'lime', '<eos>']]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 6, 2]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "captions[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['<sos>', 'the', 'blu', '+est', 'blue', 'of', 'the', '<unk>', '<eos>'],\n",
       " [0, 1, 100, 20, 4, 22, 1, 11, 6])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featurizer.to_string_features(\"the bluest blue of the posedien\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0],\n",
       "        [  1],\n",
       "        [100],\n",
       "        [ 20],\n",
       "        [  4],\n",
       "        [ 22],\n",
       "        [  1],\n",
       "        [ 11],\n",
       "        [  6]])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featurizer.to_tensor(\"the bluest blue of the posedien\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def color_phi_id(color_list, space):\n",
    "    \"\"\"\n",
    "    Function for turning a list of colors in the given space\n",
    "    (with normalization marked by \"_norm\") into a feature function.\n",
    "    \n",
    "    This is just the identity feature function, so it's kind of boring,\n",
    "    but we can imagine doing more complext things too (like fourier\n",
    "    transform). We pass space as well in case you want to do different\n",
    "    operations based on the space or only have a feature function work\n",
    "    for HSL for example\n",
    "    \n",
    "    ex:\n",
    "    color_list = [256, 0, 0] space = 'rgb'\n",
    "    color_list = [1, 0, 0] space = 'rgb_norm'\n",
    "    \"\"\"\n",
    "    return color_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def color_phi_fourier(color_list, space, resolution=3):\n",
    "    if space != \"rgb_norm\":\n",
    "        print(\"Space must be rgb_norm to use fourier transform\")\n",
    "        return None\n",
    "\n",
    "    resolution = [resolution for _ in color_list]\n",
    "    colors = np.array([color_list])\n",
    "    ranges = np.array([256, 256, 256])\n",
    "    \n",
    "    xyz = colors / 2\n",
    "\n",
    "    ax, ay, az = [np.arange(0, g) for g, r in zip(resolution, ranges)]\n",
    "    gx, gy, gz = np.meshgrid(ax, ay, az)\n",
    "\n",
    "    arg = (np.multiply.outer(xyz[:, 0], gx) +\n",
    "           np.multiply.outer(xyz[:, 1], gy) +\n",
    "           np.multiply.outer(xyz[:, 2], gz))\n",
    "    #assert arg.shape == (xyz.shape[0],) + tuple(self.resolution), arg.shape\n",
    "    repr_complex = np.exp(-2j * np.pi * (arg % 1.0)).swapaxes(1, 2).reshape((xyz.shape[0], -1))\n",
    "    result = np.hstack([repr_complex.real, repr_complex.imag]).astype(np.float32)\n",
    "    return result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 1.0000000e+00,  1.0000000e+00,  1.0000000e+00,  1.0000000e+00,\n",
       "        1.0000000e+00,  1.0000000e+00,  1.0000000e+00,  1.0000000e+00,\n",
       "        1.0000000e+00, -1.0000000e+00, -1.0000000e+00, -1.0000000e+00,\n",
       "       -1.0000000e+00, -1.0000000e+00, -1.0000000e+00, -1.0000000e+00,\n",
       "       -1.0000000e+00, -1.0000000e+00,  1.0000000e+00,  1.0000000e+00,\n",
       "        1.0000000e+00,  1.0000000e+00,  1.0000000e+00,  1.0000000e+00,\n",
       "        1.0000000e+00,  1.0000000e+00,  1.0000000e+00,  0.0000000e+00,\n",
       "        0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "        0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "       -1.2246469e-16, -1.2246469e-16, -1.2246469e-16, -1.2246469e-16,\n",
       "       -1.2246469e-16, -1.2246469e-16, -1.2246469e-16, -1.2246469e-16,\n",
       "       -1.2246469e-16,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "        0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "        0.0000000e+00,  0.0000000e+00], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "color_phi_fourier([1., 0., 0.], 'rgb_norm', resolution=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANGES_RGB = (256.0, 256.0, 256.0)\n",
    "RANGES_HSV = (361.0, 101.0, 101.0)\n",
    "C_EPSILON = 1e-4\n",
    "Shsv = True\n",
    "Sresolution = [3, 3, 3]\n",
    "def vectorize_all(colors, hsv=None, Shsv = True):\n",
    "    '''\n",
    "    >>> normalize = lambda v: np.where(v.round(2) == 0.0, 0.0, v.round(2))\n",
    "    >>> normalize(FourierVectorizer([2]).vectorize_all([(255, 0, 0), (0, 255, 255)]))\n",
    "    array([[ 1.,  1.,  1.,  1., -1., -1., -1., -1.,  0.,  0.,  0.,  0.,  0.,\n",
    "             0.,  0.,  0.],\n",
    "           [ 1., -1., -1.,  1.,  1., -1., -1.,  1.,  0.,  0.,  0.,  0.,  0.,\n",
    "             0.,  0.,  0.]], dtype=float32)\n",
    "    '''\n",
    "    if hsv is None:\n",
    "        hsv = Shsv\n",
    "\n",
    "    colors = np.array([colors])\n",
    "    assert len(colors.shape) == 3, colors.shape\n",
    "    assert colors.shape[2] == 3, colors.shape\n",
    "\n",
    "    ranges = np.array(RANGES_HSV if Shsv else RANGES_RGB)\n",
    "    print(\"ranges:\", ranges)\n",
    "    if hsv and not Shsv:\n",
    "        print(\"Converting hsv to rgb\")\n",
    "        c_hsv = colors\n",
    "        color_0_1 = skimage.color.hsv2rgb(c_hsv / (np.array(RANGES_HSV) - 1.0))\n",
    "    elif not hsv and Shsv:\n",
    "        print(\"Converting rgb to hsv\")\n",
    "        c_rgb = colors\n",
    "        color_0_1 = skimage.color.rgb2hsv(c_rgb / (np.array(RANGES_RGB) - 1.0))\n",
    "    else:\n",
    "        print(\"Just normalize\")\n",
    "        color_0_1 = colors / (ranges - 1.0)\n",
    "\n",
    "    print(\"Color (0-1):\", color_0_1)\n",
    "    # Using a Fourier representation causes colors at the boundary of the\n",
    "    # space to behave as if the space is toroidal: red = 255 would be\n",
    "    # about the same as red = 0. We don't want this...\n",
    "    xyz = color_0_1[0] / 2.0\n",
    "    if Shsv:\n",
    "        # ...*except* in the case of HSV: H is in fact a polar coordinate.\n",
    "        xyz[:, 0] *= 2.0\n",
    "\n",
    "    # ax, ay, az = [np.hstack([np.arange(0, g / 2), np.arange(r - g / 2, r)])\n",
    "    #               for g, r in zip(self.resolution, ranges)]\n",
    "    ax, ay, az = [np.arange(0, g) for g, r in zip(Sresolution, ranges)]\n",
    "    gx, gy, gz = np.meshgrid(ax, ay, az)\n",
    "\n",
    "    arg = (np.multiply.outer(xyz[:, 0], gx) +\n",
    "           np.multiply.outer(xyz[:, 1], gy) +\n",
    "           np.multiply.outer(xyz[:, 2], gz))\n",
    "    assert arg.shape == (xyz.shape[0],) + tuple(Sresolution), arg.shape\n",
    "    repr_complex = np.exp(-2j * np.pi * (arg % 1.0)).swapaxes(1, 2).reshape((xyz.shape[0], -1))\n",
    "    result = np.hstack([repr_complex.real, repr_complex.imag]).astype(np.float32)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0., 1., 1.]]])"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skimage.color.rgb2hsv([[(1., 0., 0.)]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colors: [[1.   0.5  0.25]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 1.  ,  0.71,  0.  ,  0.  , -0.71, -1.  , -1.  , -0.71,  0.  ,\n",
       "        -1.  , -0.71,  0.  ,  0.  ,  0.71,  1.  ,  1.  ,  0.71,  0.  ,\n",
       "         1.  ,  0.71,  0.  ,  0.  , -0.71, -1.  , -1.  , -0.71,  0.  ,\n",
       "         0.  , -0.71, -1.  , -1.  , -0.71,  0.  ,  0.  ,  0.71,  1.  ,\n",
       "         0.  ,  0.71,  1.  ,  1.  ,  0.71,  0.  ,  0.  , -0.71, -1.  ,\n",
       "         0.  , -0.71, -1.  , -1.  , -0.71,  0.  ,  0.  ,  0.71,  1.  ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize(color_phi_fourier([1., 0.5, 0.25], 'rgb_norm', resolution=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ranges: [256. 256. 256.]\n",
      "Converting hsv to rgb\n",
      "Color (0-1): [[[1.   0.5  0.25]]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 1.  ,  0.71,  0.  ,  0.  , -0.71, -1.  , -1.  , -0.71,  0.  ,\n",
       "        -1.  , -0.71,  0.  ,  0.  ,  0.71,  1.  ,  1.  ,  0.71,  0.  ,\n",
       "         1.  ,  0.71,  0.  ,  0.  , -0.71, -1.  , -1.  , -0.71,  0.  ,\n",
       "         0.  , -0.71, -1.  , -1.  , -0.71,  0.  ,  0.  ,  0.71,  1.  ,\n",
       "         0.  ,  0.71,  1.  ,  1.  ,  0.71,  0.  ,  0.  , -0.71, -1.  ,\n",
       "         0.  , -0.71, -1.  , -1.  , -0.71,  0.  ,  0.  ,  0.71,  1.  ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize(vectorize_all([(20, 75, 100)], hsv=True, Shsv=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ranges: [361. 101. 101.]\n",
      "Color (0-1): [[[0. 1. 1.]]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 1., -1.,  1., -1.,  1., -1.,  1., -1.,  1.,  1., -1.,  1., -1.,\n",
       "         1., -1.,  1., -1.,  1.,  1., -1.,  1., -1.,  1., -1.,  1., -1.,\n",
       "         1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.]], dtype=float32)"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize(vectorize_all([(0, 100., 100.)], hsv=True, Shsv=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = lambda v: np.where(v.round(2) == 0.0, 0.0, v.round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1., -1., -1., -1., -1.,\n",
       "        -1., -1., -1., -1., -1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.]], dtype=float32)"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize(vectorize_all([(255, 0, 0)], hsv=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 2., 4.])"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([1.0001, 2, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 4])"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5, 1. ])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[1:]/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColorFeaturizer:\n",
    "    def __init__(self, featurizer, space, **kwargs):\n",
    "        self.featurizer = featurizer\n",
    "        self.space = space\n",
    "        self.featurizer_kwargs = kwargs\n",
    "    \n",
    "    \n",
    "    def to_color_lists(self, colors, normalized):\n",
    "        # non-standard, but use the space as the variable name\n",
    "        # to access the color attribute directly\n",
    "        class_var_name = self.space\n",
    "        if normalized:\n",
    "            class_var_name = \"{}_norm\".format(class_var_name)\n",
    "        return [color.__dict__[class_var_name] for color in colors], class_var_name\n",
    "    \n",
    "    def to_tensor(self, colors, normalized=True):\n",
    "        \"\"\"\n",
    "        Convert colors to tensors where the vectors are the given by applying\n",
    "        the feature function self.featurizer to the colors \n",
    "\n",
    "        returns all colors as |colors| x |phi| matrix\n",
    "        \"\"\"\n",
    "        color_lists, space = self.to_color_lists(colors, normalized) \n",
    "        color_lists = [self.featurizer(color_list, space) for color_list in color_lists]\n",
    "        target = color_lists[0] # target is always first color\n",
    "        color_tensor = torch.tensor(color_lists, dtype=torch.float) # to get column vectors\n",
    "        return color_tensor\n",
    "    \n",
    "    def shuffle_colors(self, color_tensor):\n",
    "        \"\"\"\n",
    "        Randomly permute colors. Keep track of where the the target ends up\n",
    "        for training and error analysis\n",
    "        \"\"\"\n",
    "        permutation = torch.randperm(color_tensor.size(0))\n",
    "        target = torch.argmin(permutation).view(-1) # target always started at 0\n",
    "        return color_tensor[permutation], target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_featurizer = ColorFeaturizer(color_phi_id, \"rgb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_color_tensor = color_featurizer.to_tensor(monroe_data_train.entries[0].colors, normalized = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[hsl: [226, 81, 50], rgb [24, 73, 232],\n",
       " hsl: [283, 87, 50], rgb [176, 17, 239],\n",
       " hsl: [248, 92, 50], rgb [42, 10, 246]]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monroe_data_train.entries[0].colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 24.,  73., 232.],\n",
       "        [176.,  17., 239.],\n",
       "        [ 42.,  10., 246.]])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_color_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[176.,  17., 239.],\n",
       "         [ 42.,  10., 246.],\n",
       "         [ 24.,  73., 232.]]), tensor([2]))"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "color_featurizer.shuffle_colors(test_color_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_featurizer_fourier = ColorFeaturizer(color_phi_fourier, \"rgb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_color_tensor = color_featurizer_fourier.to_tensor(monroe_data_train.entries[0].colors, normalized = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0000e+00, -9.5694e-01,  8.3147e-01,  6.2486e-01, -8.2459e-01,\n",
       "          9.5331e-01, -2.1910e-01, -7.3565e-02,  3.5990e-01,  9.5694e-01,\n",
       "         -1.0000e+00,  9.5694e-01,  3.7132e-01, -6.2486e-01,  8.2459e-01,\n",
       "         -4.9290e-01,  2.1910e-01,  7.3565e-02,  8.3147e-01, -9.5694e-01,\n",
       "          1.0000e+00,  8.5797e-02, -3.7132e-01,  6.2486e-01, -7.2425e-01,\n",
       "          4.9290e-01, -2.1910e-01,  0.0000e+00, -2.9028e-01,  5.5557e-01,\n",
       "         -7.8074e-01,  5.6573e-01, -3.0201e-01, -9.7570e-01,  9.9729e-01,\n",
       "         -9.3299e-01, -2.9028e-01, -1.2246e-16,  2.9028e-01, -9.2851e-01,\n",
       "          7.8074e-01, -5.6573e-01, -8.7009e-01,  9.7570e-01, -9.9729e-01,\n",
       "         -5.5557e-01,  2.9028e-01,  0.0000e+00, -9.9631e-01,  9.2851e-01,\n",
       "         -7.8074e-01, -6.8954e-01,  8.7009e-01, -9.7570e-01],\n",
       "        [ 1.0000e+00, -9.7832e-01,  9.1421e-01,  9.7832e-01, -1.0000e+00,\n",
       "          9.7832e-01,  9.1421e-01, -9.7832e-01,  1.0000e+00, -5.5557e-01,\n",
       "          3.7132e-01, -1.7096e-01, -7.1573e-01,  5.5557e-01, -3.7132e-01,\n",
       "         -8.4485e-01,  7.1573e-01, -5.5557e-01, -3.8268e-01,  5.6573e-01,\n",
       "         -7.2425e-01, -1.8304e-01,  3.8268e-01, -5.6573e-01,  2.4541e-02,\n",
       "          1.8304e-01, -3.8268e-01,  0.0000e+00, -2.0711e-01,  4.0524e-01,\n",
       "         -2.0711e-01, -1.2246e-16,  2.0711e-01, -4.0524e-01,  2.0711e-01,\n",
       "          0.0000e+00, -8.3147e-01,  9.2851e-01, -9.8528e-01, -6.9838e-01,\n",
       "          8.3147e-01, -9.2851e-01, -5.3500e-01,  6.9838e-01, -8.3147e-01,\n",
       "          9.2388e-01, -8.2459e-01,  6.8954e-01,  9.8311e-01, -9.2388e-01,\n",
       "          8.2459e-01,  9.9970e-01, -9.8311e-01,  9.2388e-01],\n",
       "        [ 1.0000e+00, -9.9248e-01,  9.7003e-01,  9.9248e-01, -1.0000e+00,\n",
       "          9.9248e-01,  9.7003e-01, -9.9248e-01,  1.0000e+00,  8.7009e-01,\n",
       "         -9.2388e-01,  9.6378e-01,  8.0321e-01, -8.7009e-01,  9.2388e-01,\n",
       "          7.2425e-01, -8.0321e-01,  8.7009e-01,  5.1410e-01, -6.1523e-01,\n",
       "          7.0711e-01,  4.0524e-01, -5.1410e-01,  6.1523e-01,  2.9028e-01,\n",
       "         -4.0524e-01,  5.1410e-01,  0.0000e+00, -1.2241e-01,  2.4298e-01,\n",
       "         -1.2241e-01, -1.2246e-16,  1.2241e-01, -2.4298e-01,  1.2241e-01,\n",
       "          0.0000e+00, -4.9290e-01,  3.8268e-01, -2.6671e-01, -5.9570e-01,\n",
       "          4.9290e-01, -3.8268e-01, -6.8954e-01,  5.9570e-01, -4.9290e-01,\n",
       "         -8.5773e-01,  7.8835e-01, -7.0711e-01, -9.1421e-01,  8.5773e-01,\n",
       "         -7.8835e-01, -9.5694e-01,  9.1421e-01, -8.5773e-01]])"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_color_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.  , -0.96,  0.83,  0.62, -0.82,  0.95, -0.22, -0.07,  0.36,\n",
       "        0.96, -1.  ,  0.96,  0.37, -0.62,  0.82, -0.49,  0.22,  0.07,\n",
       "        0.83, -0.96,  1.  ,  0.09, -0.37,  0.62, -0.72,  0.49, -0.22,\n",
       "        0.  , -0.29,  0.56, -0.78,  0.57, -0.3 , -0.98,  1.  , -0.93,\n",
       "       -0.29,  0.  ,  0.29, -0.93,  0.78, -0.57, -0.87,  0.98, -1.  ,\n",
       "       -0.56,  0.29,  0.  , -1.  ,  0.93, -0.78, -0.69,  0.87, -0.98],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize(test_color_tensor[0].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[226, 81, 50]"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monroe_data_train.entries[0].colors[0].hsl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.0000000e+00, -9.5694035e-01,  8.3146960e-01,  6.2485951e-01,\n",
       "       -8.2458931e-01,  9.5330602e-01, -2.1910124e-01, -7.3564567e-02,\n",
       "        3.5989505e-01,  9.5694035e-01, -1.0000000e+00,  9.5694035e-01,\n",
       "        3.7131721e-01, -6.2485951e-01,  8.2458931e-01, -4.9289820e-01,\n",
       "        2.1910124e-01,  7.3564567e-02,  8.3146960e-01, -9.5694035e-01,\n",
       "        1.0000000e+00,  8.5797310e-02, -3.7131721e-01,  6.2485951e-01,\n",
       "       -7.2424710e-01,  4.9289820e-01, -2.1910124e-01,  0.0000000e+00,\n",
       "       -2.9028466e-01,  5.5557024e-01, -7.8073722e-01,  5.6573182e-01,\n",
       "       -3.0200595e-01, -9.7570211e-01,  9.9729043e-01, -9.3299282e-01,\n",
       "       -2.9028466e-01, -1.2246469e-16,  2.9028466e-01, -9.2850608e-01,\n",
       "        7.8073722e-01, -5.6573182e-01, -8.7008697e-01,  9.7570211e-01,\n",
       "       -9.9729043e-01, -5.5557024e-01,  2.9028466e-01,  0.0000000e+00,\n",
       "       -9.9631262e-01,  9.2850608e-01, -7.8073722e-01, -6.8954057e-01,\n",
       "        8.7008697e-01, -9.7570211e-01], dtype=float32)"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_color_tensor[0].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torchtext trial\n",
    "from torchtext.vocab import Vocab\n",
    "from torchtext import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "monroe_data_train = MonroeData(\"../data/csv/train_corpus_monroe.csv\", \"../data/entries/train_entries_monroe.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = EndingTokenizer()\n",
    "word_count = Counter()\n",
    "for entry in monroe_data_train:\n",
    "    caption_tokens = tokenizer.tokenize(entry.caption)\n",
    "    for token in caption_tokens:\n",
    "        word_count[token] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('green', 3510),\n",
       " ('the', 2925),\n",
       " ('blue', 2665),\n",
       " ('purple', 2392),\n",
       " ('~', 1994),\n",
       " ('bright', 1824),\n",
       " ('+er', 1712),\n",
       " ('not', 1483),\n",
       " ('+ish', 1452),\n",
       " ('pink', 1397),\n",
       " ('grey', 1393),\n",
       " ('dark', 1247),\n",
       " ('one', 1239),\n",
       " ('+est', 1236),\n",
       " (',', 1090),\n",
       " ('gray', 774),\n",
       " ('.', 743),\n",
       " ('yellow', 740),\n",
       " ('color', 706),\n",
       " ('light', 693),\n",
       " ('of', 655),\n",
       " ('is', 626),\n",
       " ('brown', 620),\n",
       " ('red', 582),\n",
       " ('orange', 515),\n",
       " ('dull', 496),\n",
       " ('The', 461),\n",
       " ('more', 424),\n",
       " ('a', 412),\n",
       " ('that', 333),\n",
       " ('or', 293),\n",
       " ('most', 284),\n",
       " ('it', 266),\n",
       " ('neon', 257),\n",
       " ('and', 257),\n",
       " ('with', 251),\n",
       " ('to', 250),\n",
       " ('teal', 245),\n",
       " ('like', 239),\n",
       " ('?', 222),\n",
       " ('sky', 204),\n",
       " ('lime', 185),\n",
       " ('!', 185),\n",
       " ('olive', 181),\n",
       " ('but', 179),\n",
       " ('...', 178),\n",
       " ('tan', 177),\n",
       " ('shade', 174),\n",
       " ('target', 174),\n",
       " ('blu', 171),\n",
       " (\"'s\", 168),\n",
       " ('in', 166),\n",
       " ('two', 151),\n",
       " ('this', 134),\n",
       " ('box', 129),\n",
       " (')', 127),\n",
       " ('you', 124),\n",
       " ('hot', 124),\n",
       " ('grass', 121),\n",
       " ('no', 120),\n",
       " ('oth', 119),\n",
       " ('on', 103),\n",
       " ('aqua', 103),\n",
       " ('its', 102),\n",
       " ('Purple', 98),\n",
       " ('medium', 94),\n",
       " ('purpl', 93),\n",
       " ('than', 93),\n",
       " ('muted', 93),\n",
       " ('(', 92),\n",
       " ('less', 92),\n",
       " ('click', 92),\n",
       " ('redd', 90),\n",
       " ('clos', 90),\n",
       " ('Green', 89),\n",
       " ('looking', 89),\n",
       " ('looks', 88),\n",
       " ('are', 86),\n",
       " ('pick', 83),\n",
       " ('Blue', 83),\n",
       " ('middle', 82),\n",
       " ('yes', 82),\n",
       " ('lol', 80),\n",
       " ('I', 80),\n",
       " ('same', 80),\n",
       " ('tint', 80),\n",
       " ('-', 79),\n",
       " ('i', 77),\n",
       " ('colors', 77),\n",
       " ('square', 76),\n",
       " ('Bright', 73),\n",
       " ('pale', 73),\n",
       " ('again', 73),\n",
       " ('purp', 72),\n",
       " ('as', 71),\n",
       " ('least', 70),\n",
       " ('Click', 67),\n",
       " ('mustard', 66),\n",
       " ('has', 66),\n",
       " ('Not', 65),\n",
       " ('violet', 65),\n",
       " ('Dark', 60),\n",
       " (':', 59),\n",
       " ('mauve', 59),\n",
       " ('It', 57),\n",
       " ('drab', 57),\n",
       " ('similar', 57),\n",
       " ('almost', 55),\n",
       " ('sorry', 54),\n",
       " ('barney', 54),\n",
       " ('good', 53),\n",
       " ('magenta', 51),\n",
       " ('slightly', 49),\n",
       " ('deep', 47),\n",
       " ('true', 47),\n",
       " ('rose', 47),\n",
       " ('hard', 46),\n",
       " ('lavend', 45),\n",
       " ('vibrant', 43),\n",
       " ('baby', 43),\n",
       " ('we', 42),\n",
       " ('kind', 42),\n",
       " ('nice', 41),\n",
       " ('very', 41),\n",
       " ('turquoise', 41),\n",
       " ('think', 40),\n",
       " ('NOT', 40),\n",
       " ('white', 40),\n",
       " ('army', 39),\n",
       " ('last', 38),\n",
       " ('mint', 38),\n",
       " ('Pink', 38),\n",
       " ('right', 37),\n",
       " ('see', 37),\n",
       " (\"n't\", 37),\n",
       " ('my', 37),\n",
       " ('me', 37),\n",
       " ('Gray', 36),\n",
       " ('ok', 36),\n",
       " (\"'\", 36),\n",
       " ('pea', 35),\n",
       " ('yeah', 35),\n",
       " ('Grey', 34),\n",
       " ('ones', 34),\n",
       " ('so', 34),\n",
       " ('was', 34),\n",
       " ('This', 33),\n",
       " ('all', 33),\n",
       " ('sea', 33),\n",
       " ('little', 33),\n",
       " ('puke', 33),\n",
       " ('pastel', 31),\n",
       " ('salmon', 31),\n",
       " ('2', 31),\n",
       " ('for', 30),\n",
       " ('too', 30),\n",
       " ('hint', 30),\n",
       " ('black', 30),\n",
       " ('caca', 30),\n",
       " ('job', 29),\n",
       " ('gold', 29),\n",
       " ('normal', 28),\n",
       " ('between', 28),\n",
       " ('orang', 27),\n",
       " ('Brown', 27),\n",
       " ('bit', 27),\n",
       " ('here', 27),\n",
       " ('be', 27),\n",
       " ('close', 27),\n",
       " ('beige', 27),\n",
       " ('really', 26),\n",
       " ('look', 26),\n",
       " ('Yes', 26),\n",
       " ('kinda', 26),\n",
       " ('colored', 25),\n",
       " ('', 25),\n",
       " ('royal', 25),\n",
       " ('bold', 25),\n",
       " ('concrete', 25),\n",
       " ('Yellow', 24),\n",
       " ('Color', 24),\n",
       " ('do', 24),\n",
       " ('an', 24),\n",
       " ('greens', 24),\n",
       " ('out', 24),\n",
       " ('regular', 23),\n",
       " ('three', 23),\n",
       " ('tone', 23),\n",
       " ('girl', 23),\n",
       " ('pal', 23),\n",
       " ('okay', 23),\n",
       " ('some', 23),\n",
       " ('choose', 23),\n",
       " ('great', 22),\n",
       " ('Orange', 22),\n",
       " ('have', 22),\n",
       " ('tones', 22),\n",
       " ('Its', 22),\n",
       " ('cent', 22),\n",
       " ('just', 21),\n",
       " ('stormy', 21),\n",
       " ('awesome', 21),\n",
       " ('oops', 21),\n",
       " (\"'re\", 20),\n",
       " ('blue/green', 20),\n",
       " ('day', 20),\n",
       " ('real', 20),\n",
       " ('pure', 20),\n",
       " ('time', 20),\n",
       " ('flow', 20),\n",
       " ('hue', 20),\n",
       " ('Red', 19),\n",
       " ('doing', 19),\n",
       " ('faded', 19),\n",
       " ('ocean', 19),\n",
       " ('oh', 19),\n",
       " ('guess', 19),\n",
       " ('worries', 18),\n",
       " ('them', 18),\n",
       " ('left', 18),\n",
       " ('um', 18),\n",
       " ('yellow-green', 18),\n",
       " ('wat', 18),\n",
       " ('grapes', 18),\n",
       " ('Hello', 18),\n",
       " ('blues', 18),\n",
       " ('go', 18),\n",
       " ('then', 18),\n",
       " ('d', 18),\n",
       " ('hello', 17),\n",
       " ('slate', 17),\n",
       " ('plum', 17),\n",
       " ('pumpkin', 17),\n",
       " ('others', 17),\n",
       " ('Clos', 17),\n",
       " ('peach', 17),\n",
       " ('seafoam', 17),\n",
       " ('Now', 17),\n",
       " (';', 16),\n",
       " ('ugly', 16),\n",
       " ('maybe', 16),\n",
       " ('foam', 16),\n",
       " ('bad', 16),\n",
       " ('say', 16),\n",
       " ('thats', 16),\n",
       " ('3', 16),\n",
       " ('soft', 16),\n",
       " ('Choose', 16),\n",
       " ('they', 15),\n",
       " ('your', 15),\n",
       " ('Light', 15),\n",
       " ('No', 15),\n",
       " ('Hi', 15),\n",
       " ('Dull', 15),\n",
       " ('what', 15),\n",
       " ('at', 15),\n",
       " ('about', 15),\n",
       " ('neutral', 15),\n",
       " ('second', 15),\n",
       " ('vivid', 15),\n",
       " ('would', 15),\n",
       " ('sort', 15),\n",
       " ('want', 15),\n",
       " ('lilac', 15),\n",
       " ('sand', 15),\n",
       " ('these', 14),\n",
       " ('blue-', 14),\n",
       " ('there', 14),\n",
       " ('sun', 14),\n",
       " ('well', 14),\n",
       " ('pinks', 14),\n",
       " ('thanks', 14),\n",
       " ('boy', 14),\n",
       " ('mix', 14),\n",
       " ('if', 14),\n",
       " ('natural', 14),\n",
       " ('weird', 13),\n",
       " ('brigh', 13),\n",
       " ('brightness', 13),\n",
       " ('Tan', 13),\n",
       " ('up', 13),\n",
       " ('Teal', 13),\n",
       " ('non', 13),\n",
       " ('clay', 13),\n",
       " ('tough', 13),\n",
       " ('haha', 13),\n",
       " ('fuschia', 13),\n",
       " ('dirty', 13),\n",
       " ('yep', 13),\n",
       " ('tinge', 13),\n",
       " ('primary', 13),\n",
       " ('got', 12),\n",
       " ('can', 12),\n",
       " ('green/yellow', 12),\n",
       " ('hi', 12),\n",
       " ('Magenta', 12),\n",
       " ('screen', 12),\n",
       " ('different', 12),\n",
       " ('lipstick', 12),\n",
       " ('work', 12),\n",
       " ('eith', 12),\n",
       " ('nor', 12),\n",
       " ('know', 12),\n",
       " ('fluorescent', 12),\n",
       " ('More', 12),\n",
       " ('intense', 12),\n",
       " ('blue/purple', 12),\n",
       " ('fuscia', 12),\n",
       " ('eggplant', 12),\n",
       " ('camo', 11),\n",
       " ('wrong', 11),\n",
       " ('describe', 11),\n",
       " ('grape', 11),\n",
       " ('yellow/green', 11),\n",
       " ('skies', 11),\n",
       " ('leaf', 11),\n",
       " ('fruit', 11),\n",
       " ('apple', 11),\n",
       " ('You', 11),\n",
       " ('highlight', 11),\n",
       " ('boxes', 11),\n",
       " ('purples', 11),\n",
       " ('--', 11),\n",
       " ('which', 11),\n",
       " ('makes', 11),\n",
       " ('only', 11),\n",
       " ('girly', 11),\n",
       " ('peas', 11),\n",
       " ('leaves', 11),\n",
       " ('dirt', 11),\n",
       " ('turqoise', 11),\n",
       " ('Most', 10),\n",
       " (\"'d\", 10),\n",
       " ('does', 10),\n",
       " ('shades', 10),\n",
       " ('green/blue', 10),\n",
       " ('any', 10),\n",
       " ('boring', 10),\n",
       " ('pink/purple', 10),\n",
       " ('make', 10),\n",
       " ('LOL', 10),\n",
       " ('purple-', 10),\n",
       " ('purpley', 10),\n",
       " ('brite', 10),\n",
       " ('watermelon', 10),\n",
       " ('warm', 10),\n",
       " ('though', 9),\n",
       " ('how', 9),\n",
       " ('sup', 9),\n",
       " ('without', 9),\n",
       " ('ligh', 9),\n",
       " ('mud', 9),\n",
       " ('aquamarine', 9),\n",
       " ('grays', 9),\n",
       " ('mixed', 9),\n",
       " ('cloud', 9),\n",
       " ('Thanks', 9),\n",
       " ('2nd', 9),\n",
       " ('from', 9),\n",
       " ('tree', 9),\n",
       " ('girls', 9),\n",
       " ('tinted', 9),\n",
       " ('pinky', 9),\n",
       " ('fire', 9),\n",
       " ('maroon', 9),\n",
       " ('sure', 9),\n",
       " ('Vikings', 9),\n",
       " ('hash', 9),\n",
       " ('mango', 9),\n",
       " ('rain', 9),\n",
       " ('select', 8),\n",
       " ('way', 8),\n",
       " (\"'m\", 8),\n",
       " ('blue/gray', 8),\n",
       " ('flat', 8),\n",
       " ('Olive', 8),\n",
       " ('poop', 8),\n",
       " ('dusty', 8),\n",
       " ('leaning', 8),\n",
       " ('rainy', 8),\n",
       " ('khaki', 8),\n",
       " ('soup', 8),\n",
       " ('moss', 8),\n",
       " ('could', 8),\n",
       " ('did', 8),\n",
       " ('also', 8),\n",
       " ('&', 8),\n",
       " ('wow', 8),\n",
       " ('red/pink', 8),\n",
       " ('off', 8),\n",
       " ('saturated', 8),\n",
       " ('uhm', 8),\n",
       " ('vomit', 8),\n",
       " ('matt', 8),\n",
       " ('cement', 8),\n",
       " ('subdued', 8),\n",
       " ('ugh', 8),\n",
       " ('standard', 8),\n",
       " ('gren', 8),\n",
       " ('greeen', 8),\n",
       " ('strong', 8),\n",
       " ('mixture', 8),\n",
       " ('jerseys', 8),\n",
       " ('both', 7),\n",
       " ('far', 7),\n",
       " ('block', 7),\n",
       " ('Grass', 7),\n",
       " ('am', 7),\n",
       " ('Muted', 7),\n",
       " ('purple/blue', 7),\n",
       " ('been', 7),\n",
       " ('piggy', 7),\n",
       " ('blue-green', 7),\n",
       " ('A', 7),\n",
       " ('before', 7),\n",
       " ('uh', 7),\n",
       " ('targets', 7),\n",
       " ('coral', 7),\n",
       " ('blush', 7),\n",
       " ('gloomy', 7),\n",
       " ('Sky', 7),\n",
       " ('dingy', 7),\n",
       " ('military', 7),\n",
       " ('fuchsia', 7),\n",
       " ('those', 7),\n",
       " ('thank', 7),\n",
       " ('clear', 7),\n",
       " ('yellowy', 7),\n",
       " ('eyes', 7),\n",
       " ('Has', 7),\n",
       " ('pruple', 7),\n",
       " ('blood', 7),\n",
       " ('frog', 7),\n",
       " ('undertone', 7),\n",
       " ('subtle', 7),\n",
       " ('dusky', 7),\n",
       " ('brow', 7),\n",
       " ('rouge', 7),\n",
       " ('o', 7),\n",
       " ('vikings', 7),\n",
       " ('Like', 6),\n",
       " ('Neon', 6),\n",
       " ('Lime', 6),\n",
       " ('by', 6),\n",
       " ('get', 6),\n",
       " ('b', 6),\n",
       " ('object', 6),\n",
       " ('still', 6),\n",
       " ('done', 6),\n",
       " ('purply', 6),\n",
       " ('mid', 6),\n",
       " ('seems', 6),\n",
       " ('pink/red', 6),\n",
       " ('tell', 6),\n",
       " ('years', 6),\n",
       " ('ha', 6),\n",
       " ('muddy', 6),\n",
       " ('amount', 6),\n",
       " ('grey/blue', 6),\n",
       " ('Select', 6),\n",
       " ('green-', 6),\n",
       " ('Please', 6),\n",
       " ('rich', 6),\n",
       " ('quite', 6),\n",
       " ('stands', 6),\n",
       " ('bue', 6),\n",
       " ('pink-', 6),\n",
       " ('pukey', 6),\n",
       " ('green/gray', 6),\n",
       " ('hues', 6),\n",
       " ('getting', 6),\n",
       " ('looked', 6),\n",
       " ('sage', 6),\n",
       " ('Shade', 6),\n",
       " ('purple/gray', 6),\n",
       " ('lite', 6),\n",
       " ('washed', 6),\n",
       " ('much', 6),\n",
       " ('dont', 6),\n",
       " ('Object', 6),\n",
       " ('Neutral', 6),\n",
       " ('new', 6),\n",
       " ('sorta', 6),\n",
       " ('periwinkle', 6),\n",
       " ('/', 6),\n",
       " ('now', 6),\n",
       " (\"'ve\", 6),\n",
       " ('were', 6),\n",
       " ('Lakers', 6),\n",
       " ('Good', 6),\n",
       " ('OJ', 6),\n",
       " ('yucky', 5),\n",
       " ('working', 5),\n",
       " ('puple', 5),\n",
       " ('Target', 5),\n",
       " ('squares', 5),\n",
       " ('Lol', 5),\n",
       " ('try', 5),\n",
       " ('purple/pink', 5),\n",
       " ('yea', 5),\n",
       " ('robin', 5),\n",
       " ('egg', 5),\n",
       " (\"'ll\", 5),\n",
       " ('call', 5),\n",
       " ('blue-grey', 5),\n",
       " ('tan/brown', 5),\n",
       " ('..', 5),\n",
       " ('dreary', 5),\n",
       " ('miss', 5),\n",
       " ('turqouise', 5),\n",
       " ('Sorry', 5),\n",
       " ('gray-', 5),\n",
       " ('blue-gray', 5),\n",
       " ('Okay', 5),\n",
       " ('blund', 5),\n",
       " ('fault', 5),\n",
       " ('taupe', 5),\n",
       " ('kermit', 5),\n",
       " ('gay', 5),\n",
       " ('cloudy', 5),\n",
       " ('Violet', 5),\n",
       " ('Blu', 5),\n",
       " (']', 5),\n",
       " ('Hot', 5),\n",
       " ('used', 5),\n",
       " ('version', 5),\n",
       " ('let', 5),\n",
       " ('brick', 5),\n",
       " ('probably', 5),\n",
       " ('pretty', 5),\n",
       " ('turquiose', 5),\n",
       " ('throw', 5),\n",
       " ('our', 5),\n",
       " ('lilacs', 5),\n",
       " ('tru', 5),\n",
       " ('face', 5),\n",
       " ('touch', 5),\n",
       " ('options', 5),\n",
       " ('gray/green', 5),\n",
       " ('engine', 5),\n",
       " ('storm', 5),\n",
       " ('pin', 5),\n",
       " ('gre', 5),\n",
       " ('fun', 5),\n",
       " ('p', 5),\n",
       " ('That', 5),\n",
       " ('type', 5),\n",
       " ('tann', 5),\n",
       " ('summ', 5),\n",
       " ('tires', 5),\n",
       " ('ballet', 5),\n",
       " ('york', 5),\n",
       " ('mets', 5),\n",
       " ('cobalt', 5),\n",
       " ('purple-y', 5),\n",
       " ('greys', 5),\n",
       " ('purple-gray', 5),\n",
       " ('y', 5),\n",
       " ('gross', 5),\n",
       " ('blue-purple', 5),\n",
       " ('please', 5),\n",
       " ('urine', 5),\n",
       " ('description', 4),\n",
       " ('Haha', 4),\n",
       " ('classic', 4),\n",
       " ('grey-green', 4),\n",
       " ('grey-blue', 4),\n",
       " ('actually', 4),\n",
       " ('something', 4),\n",
       " ('problem', 4),\n",
       " ('orange/red', 4),\n",
       " ('Are', 4),\n",
       " ('Very', 4),\n",
       " ('toned', 4),\n",
       " ('green-yellow', 4),\n",
       " ('Kind', 4),\n",
       " ('East', 4),\n",
       " ('grey/green', 4),\n",
       " ('blue/grey', 4),\n",
       " ('steel', 4),\n",
       " ('matte', 4),\n",
       " ('doesnt', 4),\n",
       " ('silv', 4),\n",
       " ('purple-grey', 4),\n",
       " ('trying', 4),\n",
       " ('sandy', 4),\n",
       " ('combination', 4),\n",
       " ('undertones', 4),\n",
       " ('poo', 4),\n",
       " ('when', 4),\n",
       " ('Looks', 4),\n",
       " ('plain', 4),\n",
       " ('brown/tan', 4),\n",
       " ('Dirty', 4),\n",
       " ('rust', 4),\n",
       " ('gj', 4),\n",
       " ('Cyan', 4),\n",
       " ('Middle', 4),\n",
       " ('Dusty', 4),\n",
       " ('paint', 4),\n",
       " ('being', 4),\n",
       " ('come', 4),\n",
       " ('yup', 4),\n",
       " ('crayon', 4),\n",
       " ('someone', 4),\n",
       " ('mean', 4),\n",
       " ('slight', 4),\n",
       " ('omg', 4),\n",
       " ('Think', 4),\n",
       " ('evening', 4),\n",
       " ('colorful', 4),\n",
       " ('combo', 4),\n",
       " ('metallic', 4),\n",
       " ('np', 4),\n",
       " ('jade', 4),\n",
       " ('flourescent', 4),\n",
       " ('Rose', 4),\n",
       " ('traditional', 4),\n",
       " ('cadet', 4),\n",
       " ('meets', 4),\n",
       " ('green/brown', 4),\n",
       " ('yelow', 4),\n",
       " ('flavor', 4),\n",
       " ('citrus', 4),\n",
       " ('Ok', 4),\n",
       " ('Same', 4),\n",
       " ('Hey', 4),\n",
       " ('Um', 4),\n",
       " ('cornflow', 4),\n",
       " ('tiffany', 4),\n",
       " ('dimm', 4),\n",
       " ('sunshine', 4),\n",
       " ('teenagers', 4),\n",
       " ('room', 4),\n",
       " ('purple-pink', 4),\n",
       " ('MOST', 4),\n",
       " ('[', 4),\n",
       " ('Aqua', 4),\n",
       " ('indigo', 4),\n",
       " ('Lavendar', 4),\n",
       " ('purplr', 4),\n",
       " ('choice', 4),\n",
       " ('hmm', 4),\n",
       " ('should', 4),\n",
       " ('im', 4),\n",
       " ('partn', 4),\n",
       " ('god', 4),\n",
       " ('man', 4),\n",
       " ('remaining', 4),\n",
       " ('n', 4),\n",
       " ('during', 4),\n",
       " ('williamsburg', 4),\n",
       " ('>', 4),\n",
       " ('Was', 4),\n",
       " ('yellows', 4),\n",
       " ('Medium', 3),\n",
       " ('One', 3),\n",
       " ('Thank', 3),\n",
       " ('rock', 3),\n",
       " ('Box', 3),\n",
       " ('Some', 3),\n",
       " ('hehe', 3),\n",
       " ('copp', 3),\n",
       " ('descriptions', 3),\n",
       " ('golden', 3),\n",
       " ('jungle', 3),\n",
       " ('messed', 3),\n",
       " ('supposed', 3),\n",
       " ('ca', 3),\n",
       " ('So', 3),\n",
       " ('down', 3),\n",
       " ('turquise', 3),\n",
       " ('ash', 3),\n",
       " ('hey', 3),\n",
       " ('range', 3),\n",
       " ('theres', 3),\n",
       " ('BLue', 3),\n",
       " ('keep', 3),\n",
       " ('togeth', 3),\n",
       " ('Clo', 3),\n",
       " ('end', 3),\n",
       " ('grey/purple', 3),\n",
       " ('gray-green', 3),\n",
       " ('allowed', 3),\n",
       " ('Hmmm', 3),\n",
       " ('toddl', 3),\n",
       " ('sunny', 3),\n",
       " ('Last', 3),\n",
       " ('side', 3),\n",
       " ('cherry', 3),\n",
       " ('team', 3),\n",
       " ('midrange', 3),\n",
       " ('non-target', 3),\n",
       " ('BRIGHT', 3),\n",
       " ('murky', 3),\n",
       " ('dk', 3),\n",
       " ('food', 3),\n",
       " ('lemon', 3),\n",
       " ('grey/brown', 3),\n",
       " ('Mauve', 3),\n",
       " ('l', 3),\n",
       " ('Turquoise', 3),\n",
       " ('purple/lavend', 3),\n",
       " ('yay', 3),\n",
       " ('considered', 3),\n",
       " ('feminine', 3),\n",
       " ('likely', 3),\n",
       " ('sense', 3),\n",
       " ('leafy', 3),\n",
       " ('associated', 3),\n",
       " ('question', 3),\n",
       " ('difference', 3),\n",
       " ('often', 3),\n",
       " ('sad', 3),\n",
       " ('cyan', 3),\n",
       " ('round', 3),\n",
       " ('urple', 3),\n",
       " ('Slightly', 3),\n",
       " ('brown/gray', 3),\n",
       " ('youre', 3),\n",
       " ('=', 3),\n",
       " ('Again', 3),\n",
       " ('bland', 3),\n",
       " ('toned-down', 3),\n",
       " ('beautiful', 3),\n",
       " ('hmmm', 3),\n",
       " ('cheeks', 3),\n",
       " ('elephant', 3),\n",
       " ('lavendar', 3),\n",
       " ('putty', 3),\n",
       " ('us', 3),\n",
       " ('blind', 3),\n",
       " ('brown/green', 3),\n",
       " ('hate', 3),\n",
       " ('Highlight', 3),\n",
       " ('Girly', 3),\n",
       " ('juice', 3),\n",
       " ('S', 3),\n",
       " ('grey-', 3),\n",
       " ('hunt', 3),\n",
       " ('Barney', 3),\n",
       " ('dinosaur', 3),\n",
       " ('red-', 3),\n",
       " ('purple-blue', 3),\n",
       " ('brighter/light', 3),\n",
       " ('typically', 3),\n",
       " ('candy', 3),\n",
       " ('perfect', 3),\n",
       " ('MTG', 3),\n",
       " ('happens', 3),\n",
       " ('boys', 3),\n",
       " ('violets', 3),\n",
       " ('towards', 3),\n",
       " ('wine', 3),\n",
       " ('frogs', 3),\n",
       " ('ninja', 3),\n",
       " ('turtle', 3),\n",
       " ('slippers', 3),\n",
       " ('lizzard', 3),\n",
       " ('dolphin', 3),\n",
       " ('forum', 3),\n",
       " ('lmao', 3),\n",
       " ('ruby', 3),\n",
       " ('BLUE', 3),\n",
       " ('uhh', 3),\n",
       " ('flatt', 3),\n",
       " ('funny', 3),\n",
       " ('True', 3),\n",
       " ('typical', 3),\n",
       " ('playing', 3),\n",
       " ('navy', 3),\n",
       " ('shock', 3),\n",
       " ('next', 3),\n",
       " ('flashy', 3),\n",
       " ('There', 3),\n",
       " ('yello', 3),\n",
       " ('might', 3),\n",
       " ('purple/grey', 3),\n",
       " ('shamrock', 3),\n",
       " ('-P', 3),\n",
       " ('Dodgers', 3),\n",
       " ('Cubs', 3),\n",
       " ('uniforms', 3),\n",
       " (\"'feminine\", 3),\n",
       " ('greyist', 3),\n",
       " ('skin', 3),\n",
       " ('Oh', 3),\n",
       " ('actual', 3),\n",
       " ('person', 3),\n",
       " ('halloween', 3),\n",
       " ('money', 3),\n",
       " ('take', 3),\n",
       " ('speak', 3),\n",
       " ('going', 3),\n",
       " ('suck', 3),\n",
       " ('cool', 3),\n",
       " ('Great', 3),\n",
       " ('Partn', 3),\n",
       " ('We', 3),\n",
       " ('Correct', 3),\n",
       " ('guy', 3),\n",
       " ('burnt', 3),\n",
       " ('crap', 3),\n",
       " ('drabb', 3),\n",
       " ('Mud', 2),\n",
       " ('Camo', 2),\n",
       " ('shaded', 2),\n",
       " ('Plain', 2),\n",
       " ('YELLOW', 2),\n",
       " ('grey-brown', 2),\n",
       " ('grey-purple', 2),\n",
       " ('green-grey', 2),\n",
       " ('puprle', 2),\n",
       " ('DARK', 2),\n",
       " ('YOU', 2),\n",
       " ('puirple', 2),\n",
       " ('Navy', 2),\n",
       " ('Second', 2),\n",
       " ('Yeah', 2),\n",
       " ('idea', 2),\n",
       " ('kidding', 2),\n",
       " ('Regular', 2),\n",
       " ('gray/blue', 2),\n",
       " ('give', 2),\n",
       " ('D', 2),\n",
       " ('rpurple', 2),\n",
       " ('basic', 2),\n",
       " ('green-blue', 2),\n",
       " ('sec', 2),\n",
       " ('ov', 2),\n",
       " ('thought', 2),\n",
       " ('feel', 2),\n",
       " ('electric', 2),\n",
       " ('few', 2),\n",
       " ('ah', 2),\n",
       " ('purple/amethyst', 2),\n",
       " ('icky', 2),\n",
       " ('Army', 2),\n",
       " ('pink-purple', 2),\n",
       " ('hte', 2),\n",
       " ('orchid', 2),\n",
       " ('pea-', 2),\n",
       " ('caribbean', 2),\n",
       " ('birght', 2),\n",
       " ('match', 2),\n",
       " ('dim', 2),\n",
       " ('To', 2),\n",
       " ('red-orange', 2),\n",
       " ('monitor', 2),\n",
       " ('Ha', 2),\n",
       " ('made', 2),\n",
       " ('tomato', 2),\n",
       " ('greenish-blu', 2),\n",
       " ('closes', 2),\n",
       " ('brightest/light', 2),\n",
       " ('in-between', 2),\n",
       " ('lighter/bright', 2),\n",
       " ('green/neon', 2),\n",
       " ('purple/not', 2),\n",
       " ('mellow', 2),\n",
       " ('spectrum', 2),\n",
       " ('In', 2),\n",
       " ('Stormy', 2),\n",
       " ('Does', 2),\n",
       " ('Yep', 2),\n",
       " ('east', 2),\n",
       " ('rainbow', 2),\n",
       " ('show', 2),\n",
       " ('differently', 2),\n",
       " ('Also', 2),\n",
       " ('ok.', 2),\n",
       " ('Purpl', 2),\n",
       " ('Purple-grey', 2),\n",
       " ('Green-grey', 2),\n",
       " ('Blue-green', 2),\n",
       " ('Redd', 2),\n",
       " ('pur', 2),\n",
       " ('stop', 2),\n",
       " ('sign', 2),\n",
       " ('oranges', 2),\n",
       " ('nope', 2),\n",
       " ('airport', 2),\n",
       " ('carpet', 2),\n",
       " ('seawat', 2),\n",
       " ('pink..', 2),\n",
       " ('purple..', 2),\n",
       " ('blue..', 2),\n",
       " ('grass-like', 2),\n",
       " ('Non-targets', 2),\n",
       " ('veggie', 2),\n",
       " ('comes', 2),\n",
       " ('canary', 2),\n",
       " ('neith', 2),\n",
       " ('orange/brown', 2),\n",
       " ('orangey', 2),\n",
       " ('Dingy', 2),\n",
       " ('Yellow/Green', 2),\n",
       " ('Gold', 2),\n",
       " ('since', 2),\n",
       " ('pepto', 2),\n",
       " ('thing', 2),\n",
       " ('battleship', 2),\n",
       " ('St.', 2),\n",
       " ('Patrick', 2),\n",
       " ('resembles', 2),\n",
       " ('trunk', 2),\n",
       " ('swim', 2),\n",
       " ('easy', 2),\n",
       " ('seeing', 2),\n",
       " ('aft', 2),\n",
       " ('using', 2),\n",
       " ('sunset', 2),\n",
       " ('may', 2),\n",
       " ('had', 2),\n",
       " ('wear', 2),\n",
       " ('royalty', 2),\n",
       " ('shadow', 2),\n",
       " ('suppose', 2),\n",
       " ('pops', 2),\n",
       " ('earli', 2),\n",
       " ('nah', 2),\n",
       " ('reference', 2),\n",
       " ('whales', 2),\n",
       " ('OF', 2),\n",
       " ('spring', 2),\n",
       " ('appliance', 2),\n",
       " ('bus', 2),\n",
       " ('bllue', 2),\n",
       " ('blueist', 2),\n",
       " ('blame', 2),\n",
       " ('=D', 2),\n",
       " ('love', 2),\n",
       " ('why', 2),\n",
       " ('Florida', 2),\n",
       " ('row', 2),\n",
       " ('marine', 2),\n",
       " ('relaxing', 2),\n",
       " ('smell', 2),\n",
       " ('smog', 2),\n",
       " ('pee', 2),\n",
       " ('sick', 2),\n",
       " ('w', 2),\n",
       " ('woot', 2),\n",
       " ('whew', 2),\n",
       " ('sigh', 2),\n",
       " ('idk', 2),\n",
       " ('u2', 2),\n",
       " ('red/orange', 2),\n",
       " ('gra', 2),\n",
       " ('Nice', 2),\n",
       " ('azure', 2),\n",
       " ('Lipstick', 2),\n",
       " ('Sunshine', 2),\n",
       " ('distinctively', 2),\n",
       " ('correct', 2),\n",
       " ('*purple', 2),\n",
       " ('tan-', 2),\n",
       " ('enough', 2),\n",
       " ('old', 2),\n",
       " ('Kinda', 2),\n",
       " ('horrible', 2),\n",
       " ('fushia', 2),\n",
       " ('violet/purple', 2),\n",
       " ('gray/purple', 2),\n",
       " ('matching', 2),\n",
       " ('closet', 2),\n",
       " ('even', 2),\n",
       " ('red/brown', 2),\n",
       " ('yellow-', 2),\n",
       " ('bark', 2),\n",
       " ('bubblegum', 2),\n",
       " ('yell', 2),\n",
       " ('dul', 2),\n",
       " ('gree', 2),\n",
       " ('yel', 2),\n",
       " ('org', 2),\n",
       " ('brt', 2),\n",
       " ('fair', 2),\n",
       " ('meant', 2),\n",
       " ('flowers', 2),\n",
       " ('grayets', 2),\n",
       " ('apples', 2),\n",
       " ('usually', 2),\n",
       " ('completely', 2),\n",
       " ('once', 2),\n",
       " ('leath', 2),\n",
       " ('wood', 2),\n",
       " ('pap', 2),\n",
       " ('bag', 2),\n",
       " ('anoth', 2),\n",
       " ('tinged', 2),\n",
       " ('inbetween', 2),\n",
       " ('oragne', 2),\n",
       " ('Tiffany', 2),\n",
       " ('company', 2),\n",
       " ('Mostly', 2),\n",
       " ('oilve', 2),\n",
       " ('start', 2),\n",
       " ('mind', 2),\n",
       " ('starts', 2),\n",
       " ...]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_count.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt_vocab = Vocab(word_count, specials=['<s>' '</s>', '<unk>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt_vocab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_field = data.Field(init_token='<s>', eos_token='</s>', tokenize=tokenizer.tokenize, lower=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_captions = [de.caption for de in monroe_data_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "zip argument #1 must support iteration",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-48a4e6147a9a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_captions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfields\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_field\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/benjaminnewman/miniconda3/lib/python3.6/site-packages/torchtext/data/example.py\u001b[0m in \u001b[0;36mfromlist\u001b[0;34m(cls, data, fields)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfromlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfields\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfield\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfields\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfield\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: zip argument #1 must support iteration"
     ]
    }
   ],
   "source": [
    "data.Example.fromlist(train_captions, fields=data_field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ttdataset = data.TabularDataset(path=\"../data/csv/train_corpus_monroe.csv\", format=\"csv\", fields=[('caption', data_field)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_field.build_vocab(train_ttdataset, min_freq=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "expected sequence of length 3 at dim 1 (got 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-86befbcccd40>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata_field\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumericalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"<s>\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"the\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"<unk>\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"blue\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"one\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"</s>\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/benjaminnewman/miniconda3/lib/python3.6/site-packages/torchtext/data/field.py\u001b[0m in \u001b[0;36mnumericalize\u001b[0;34m(self, arr, device)\u001b[0m\n\u001b[1;32m    321\u001b[0m                 \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostprocessing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 323\u001b[0;31m         \u001b[0mvar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequential\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_first\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: expected sequence of length 3 at dim 1 (got 5)"
     ]
    }
   ],
   "source": [
    "data_field.numericalize([\"<s>\", \"the\", \"<unk>\", \"blue\", \"one\", \"</s>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_field.build_vocab(train_captions, min_freq=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'Field' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-c6423f50d4d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_field\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'Field' has no len()"
     ]
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 7, 13,  5,  4, 20, 14,  6, 21,  5,  6,  4, 18,  8, 15,  5,  4,  9, 10,\n",
       "          5]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_field.numericalize(\"the darker blue one\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_featurizer(self, data_entries, construct_idx=True):\n",
    "    \"\"\"\n",
    "    data_entries is of type MonroeData.\n",
    "    \"\"\"\n",
    "    self.word_count = Counter()\n",
    "    for entry in data_entries:\n",
    "        caption_tokens = self.tokenizer.tokenize(entry.caption)\n",
    "        for token in caption_tokens:\n",
    "            self.word_count[token] += 1\n",
    "\n",
    "    if construct_idx:\n",
    "        # just construct the index so we don't have to worry about calling\n",
    "        # anything with the construct=True argument to to_string_features\n",
    "        for entry in data_entries:\n",
    "            _ = self.to_string_features(entry.caption, construct=True)\n",
    "        self.initialized = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(tokenizer=tokenizer.tokenize, lowercase=True, min_df=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vectorizer.fit_transform(train_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15665, 994)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
