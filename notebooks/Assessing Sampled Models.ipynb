{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so we can use packages from parent directory\n",
    "import sys\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code copied from example experiments.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from monroe_data import MonroeData, MonroeDataEntry, Color # last two for reading pkl file\n",
    "import caption_featurizers\n",
    "from color_featurizers import ColorFeaturizer, color_phi_fourier\n",
    "from models import LiteralListener, LiteralSpeaker, ImaginativeListener, CaptionEncoder, CaptionGenerator, ColorGenerator\n",
    "from evaluation import score_model, delta_e_dist, Speaker, Score\n",
    "from experiment import FeatureHandler, evaluate_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import experiment\n",
    "importlib.reload(experiment)\n",
    "from experiment import FeatureHandler, evaluate_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix=\"../\"\n",
    "train_data = MonroeData(prefix + \"data/csv/train_corpus_monroe.csv\", prefix + \"data/entries/train_entries_monroe.pkl\")\n",
    "dev_data_synth  = MonroeData(prefix + \"data/csv/dev_corpus_synth_10fold.csv\", prefix + \"data/entries/dev_corpus_synth_10fold.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_synth  = MonroeData(prefix + \"data/csv/test_corpus_synth_10fold.csv\", prefix + \"data/entries/test_corpus_synth_10fold.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def composite_score(eval_df, speaker=\"gameid\"):\n",
    "    mean_scores = eval_df.groupby(speaker).numOutcome.mean()\n",
    "    mean_numCleanWords = eval_df.groupby(speaker).numCleanWords.mean()\n",
    "    mean_clkTime = eval_df.groupby(speaker).clkTime.mean()\n",
    "    true_scores = mean_scores / mean_clkTime / mean_numCleanWords\n",
    "    max_score = true_scores.max()\n",
    "    true_scores /= max_score # normalize the scores\n",
    "    return true_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Imaginative Listener\n",
    "def imaginative_listener(model_file=\"../model/imaginative_listener_with_distractors_linear100hd5epoch_GLOVE_MSE.params\"):\n",
    "    print(\"Initializing featurizers\")\n",
    "    caption_phi = caption_featurizers.CaptionFeaturizer(tokenizer=caption_featurizers.EndingTokenizer)\n",
    "    color_phi = ColorFeaturizer(color_phi_fourier, \"rgb\", normalized=True)\n",
    "\n",
    "    def target_color_target(data_entry):\n",
    "        return np.array(data_entry.colors[0].rgb_norm)\n",
    "\n",
    "    feature_handler = FeatureHandler(train_data, test_data_synth, caption_phi, color_phi, target_fn=target_color_target,\n",
    "                                randomized_colors=False) #using TEST data now :) \n",
    "\n",
    "    print(\"Obtaining training features\") # get features even if you're runnning the pretrained model for example\n",
    "    #train_features = feature_handler.train_features()\n",
    "    #train_targets = feature_handler.train_targets()\n",
    "\n",
    "    imaginative_model = ImaginativeListener(ColorGenerator, criterion=torch.nn.CosineEmbeddingLoss,\n",
    "                            optimizer=torch.optim.Adam, lr=0.004, num_epochs=5)\n",
    "\n",
    "    # Creating model\n",
    "    MSELossSum = lambda: nn.MSELoss(reduction='sum') # sorry for this ugliness..... but this is me passing a parameter to the loss func\n",
    "    imaginative_model = ImaginativeListener(ColorGenerator, criterion=MSELossSum,\n",
    "                                optimizer=torch.optim.Adam, lr=0.001, num_epochs=5, use_color=True)\n",
    "    imaginative_model.init_model(embed_dim=100, hidden_dim=50, vocab_size=feature_handler.caption_featurizer.caption_indexer.size,\n",
    "                    color_in_dim=54, color_hidden_dim=50, weight_matrix=caption_featurizers.get_pretrained_glove(feature_handler.caption_featurizer.caption_indexer.idx2word.items(), 100, prefix=True))\n",
    "\n",
    "    imaginative_model.load_model(model_file)\n",
    "        \n",
    "    print(\"Evaluating model\")\n",
    "    output_to_score_de = lambda outputs, targets: np.array([delta_e_dist(outputs[i], targets[i]) for i in range(len(targets))])\n",
    "    # we want to score based on the model's predictions at the TARGET indices not listener clicked indices,\n",
    "    # so we change the feature_handler's target function to do that:\n",
    "    my_score_model = partial(score_model, speaker=Speaker.BY_GAME_ID_COND, return_df=True, score=Score.COMPOSITE)\n",
    "    result = evaluate_model(test_data_synth, feature_handler, imaginative_model, output_to_score_de, my_score_model, accuracy=False)\n",
    "    \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_imaginative_listener_samples():\n",
    "    model_directory = \"../imaginative_listener_samples\"\n",
    "    num_samples = 10\n",
    "    aggregate_correlations = []\n",
    "    close_correlations = []\n",
    "    split_correlations = []\n",
    "    far_correlations = []\n",
    "\n",
    "    aggregate_accuracies = []\n",
    "    close_accuracies = []\n",
    "    split_accuracies = []\n",
    "    far_accuracies = []\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        print(\"Evaluating sample #{}\".format(i))\n",
    "        _, imaginative_listener_eval = imaginative_listener(\"{}/sample_{}.params\".format(model_directory, i))\n",
    "        il_true_scores = imaginative_listener_eval.groupby('gameid').numOutcome.mean()\n",
    "        il_model_scores = imaginative_listener_eval.groupby('gameid').model_scores.mean()\n",
    "        il_true_scores_composite = composite_score(imaginative_listener_eval)\n",
    "\n",
    "        aggregate_correlations.append(stats.pearsonr(il_model_scores, il_true_scores_composite))\n",
    "        # arbitrarily say we get it right if we assign a majority of the probability mass to it\n",
    "        #aggregate_accuracies.append(sum(imaginative_listener_eval.model_scores > 0.5)/len(imaginative_listener_eval.model_scores))\n",
    "        aggregate_accuracies.append(np.mean(imaginative_listener_eval.model_scores))\n",
    "\n",
    "        # separate out conditions\n",
    "        imaginative_listener_close = imaginative_listener_eval[imaginative_listener_eval.condition == \"close\"]\n",
    "        imaginative_listener_split = imaginative_listener_eval[imaginative_listener_eval.condition == \"split\"]\n",
    "        imaginative_listener_far = imaginative_listener_eval[imaginative_listener_eval.condition == \"far\"]\n",
    "\n",
    "        imaginative_listener_close_true_scores = imaginative_listener_close.groupby('gameid').numOutcome.mean()\n",
    "        imaginative_listener_close_model_scores = imaginative_listener_close.groupby('gameid').model_scores.mean()\n",
    "\n",
    "        imaginative_listener_split_true_scores =  imaginative_listener_split.groupby('gameid').numOutcome.mean()\n",
    "        imaginative_listener_split_model_scores = imaginative_listener_split.groupby('gameid').model_scores.mean()\n",
    "\n",
    "        imaginative_listener_far_true_scores =  imaginative_listener_far.groupby('gameid').numOutcome.mean()\n",
    "        imaginative_listener_far_model_scores = imaginative_listener_far.groupby('gameid').model_scores.mean()\n",
    "\n",
    "        # turn true scores to composite, gricean scores\n",
    "        imaginative_listener_close_composite = composite_score(imaginative_listener_close)\n",
    "        imaginative_listener_split_composite = composite_score(imaginative_listener_split)\n",
    "        imaginative_listener_far_composite   = composite_score(imaginative_listener_far)\n",
    "\n",
    "        close_correlations.append(stats.pearsonr(imaginative_listener_close_composite, imaginative_listener_close_model_scores))\n",
    "        split_correlations.append(stats.pearsonr(imaginative_listener_split_composite, imaginative_listener_split_model_scores))\n",
    "        far_correlations.append(stats.pearsonr(imaginative_listener_far_composite, imaginative_listener_far_model_scores))\n",
    "\n",
    "    #     close_accuracies.append(sum(imaginative_listener_close_model_scores > 0.5)/len(imaginative_listener_close_model_scores))\n",
    "    #     split_accuracies.append(sum(imaginative_listener_split_model_scores > 0.5)/len(imaginative_listener_split_model_scores))\n",
    "    #     far_accuracies.append(sum(imaginative_listener_far_model_scores > 0.5)/len(imaginative_listener_far_model_scores))\n",
    "        close_accuracies.append(np.mean(imaginative_listener_close_model_scores))\n",
    "        split_accuracies.append(np.mean(imaginative_listener_split_model_scores))\n",
    "        far_accuracies.append(np.mean(imaginative_listener_far_model_scores))\n",
    "        \n",
    "        print(\"Most recent stats:\")\n",
    "        print(\"agg acc:\", aggregate_accuracies[-1])\n",
    "        print(\"clo acc:\", close_accuracies[-1])\n",
    "        print(\"spl acc:\", split_accuracies[-1])\n",
    "        print(\"far acc:\", far_accuracies[-1])\n",
    "        print(\"agg cor:\", aggregate_correlations[-1])\n",
    "        print(\"clo cor:\", close_correlations[-1])\n",
    "        print(\"spl cor:\", split_correlations[-1])\n",
    "        print(\"far cor:\", far_correlations[-1])\n",
    "        \n",
    "    return {\"aggregate_accuracies\": aggregate_accuracies,\n",
    "            \"close_accuracies\": close_accuracies,\n",
    "            \"split_accuracies\": split_accuracies,\n",
    "            \"far_accuracies\": far_accuracies,\n",
    "            \"aggregate_correlations\": aggregate_correlations,\n",
    "            \"close_correlations\":close_correlations,\n",
    "            \"far_correlations\":far_correlations,\n",
    "            \"split_correlations\": split_correlations}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating sample #0\n",
      "Initializing featurizers\n",
      "Obtaining training features\n",
      "Evaluating model\n",
      "Got here to composite score\n",
      "Most recent stats:\n",
      "agg acc: 23.98516954890258\n",
      "clo acc: 18.27353323876809\n",
      "spl acc: 19.544536012224782\n",
      "far acc: 34.14159079769889\n",
      "agg cor: (-0.886422302978764, 4.250960310734327e-178)\n",
      "clo cor: (-0.455535609378521, 2.0635307187084423e-28)\n",
      "spl cor: (-0.5473674197436782, 1.3343968892457512e-42)\n",
      "far cor: (-0.8657290229509013, 3.1337385241772896e-160)\n",
      "Evaluating sample #1\n",
      "Initializing featurizers\n",
      "Obtaining training features\n",
      "Evaluating model\n",
      "Got here to composite score\n",
      "Most recent stats:\n",
      "agg acc: 24.211091965944536\n",
      "clo acc: 18.65899781108387\n",
      "spl acc: 19.767035392086786\n",
      "far acc: 34.21259842311515\n",
      "agg cor: (-0.8860109120142069, 1.0393073393501362e-177)\n",
      "clo cor: (-0.44432579193215566, 5.887339051429764e-27)\n",
      "spl cor: (-0.5267565591108372, 4.9684543265154734e-39)\n",
      "far cor: (-0.8675670970509768, 1.079527020742103e-161)\n",
      "Evaluating sample #2\n",
      "Initializing featurizers\n",
      "Obtaining training features\n",
      "Evaluating model\n",
      "Got here to composite score\n",
      "Most recent stats:\n",
      "agg acc: 23.95348429830869\n",
      "clo acc: 18.437027023551213\n",
      "spl acc: 19.35961898023499\n",
      "far acc: 34.067678865462305\n",
      "agg cor: (-0.8919397009031778, 1.8685858962595625e-183)\n",
      "clo cor: (-0.4667281491674002, 6.407980536865597e-30)\n",
      "spl cor: (-0.5717604401511618, 3.6777693898206863e-47)\n",
      "far cor: (-0.8686362282684238, 1.4867365072500277e-162)\n",
      "Evaluating sample #3\n",
      "Initializing featurizers\n",
      "Obtaining training features\n",
      "Evaluating model\n",
      "Got here to composite score\n",
      "Most recent stats:\n",
      "agg acc: 24.018430904966912\n",
      "clo acc: 18.49341611855365\n",
      "spl acc: 19.4753261426775\n",
      "far acc: 34.09191124694347\n",
      "agg cor: (-0.8866139971776391, 2.7993650643129003e-178)\n",
      "clo cor: (-0.43810811058861493, 3.581899902245579e-26)\n",
      "spl cor: (-0.5296433756282287, 1.624662845037862e-39)\n",
      "far cor: (-0.8647559216076199, 1.8270776453110162e-159)\n",
      "Evaluating sample #4\n",
      "Initializing featurizers\n",
      "Obtaining training features\n",
      "Evaluating model\n",
      "Got here to composite score\n",
      "Most recent stats:\n",
      "agg acc: 24.17482971481473\n",
      "clo acc: 18.62743453645803\n",
      "spl acc: 19.752563626397155\n",
      "far acc: 34.14945435477963\n",
      "agg cor: (-0.8894263476542338, 5.585736691209525e-181)\n",
      "clo cor: (-0.45758906376919695, 1.1018524984903104e-28)\n",
      "spl cor: (-0.5325362340054747, 5.242969797755635e-40)\n",
      "far cor: (-0.869522995700688, 2.8335708940723594e-163)\n",
      "Evaluating sample #5\n",
      "Initializing featurizers\n",
      "Obtaining training features\n",
      "Evaluating model\n",
      "Got here to composite score\n",
      "Most recent stats:\n",
      "agg acc: 24.143522513533853\n",
      "clo acc: 18.643209041814377\n",
      "spl acc: 19.732187550885364\n",
      "far acc: 34.06440540427714\n",
      "agg cor: (-0.8811402706142353, 3.1832120315640084e-173)\n",
      "clo cor: (-0.4513571761678967, 7.3012410470162385e-28)\n",
      "spl cor: (-0.5007576193660417, 7.324464823278248e-35)\n",
      "far cor: (-0.8612623408930862, 9.16923335417871e-157)\n",
      "Evaluating sample #6\n",
      "Initializing featurizers\n",
      "Obtaining training features\n",
      "Evaluating model\n",
      "Got here to composite score\n",
      "Most recent stats:\n",
      "agg acc: 24.06136433254021\n",
      "clo acc: 18.56301687867534\n",
      "spl acc: 19.482940637388445\n",
      "far acc: 34.14122255837141\n",
      "agg cor: (-0.8828030354822461, 9.859024596492022e-175)\n",
      "clo cor: (-0.4389762719390835, 2.7898971710967474e-26)\n",
      "spl cor: (-0.529100039751234, 2.0067346509033875e-39)\n",
      "far cor: (-0.8659216681301237, 2.2068141227299776e-160)\n",
      "Evaluating sample #7\n",
      "Initializing featurizers\n",
      "Obtaining training features\n",
      "Evaluating model\n",
      "Got here to composite score\n",
      "Most recent stats:\n",
      "agg acc: 23.9379978094873\n",
      "clo acc: 18.382157579668977\n",
      "spl acc: 19.335616835781217\n",
      "far acc: 34.09880858608765\n",
      "agg cor: (-0.8858832216885367, 1.3707229566334728e-177)\n",
      "clo cor: (-0.4398237282512436, 2.1844838924650963e-26)\n",
      "spl cor: (-0.5283483825166942, 2.686037528842471e-39)\n",
      "far cor: (-0.8721234305787758, 2.0436151820523473e-165)\n",
      "Evaluating sample #8\n",
      "Initializing featurizers\n",
      "Obtaining training features\n",
      "Evaluating model\n",
      "Got here to composite score\n",
      "Most recent stats:\n",
      "agg acc: 24.076479484977696\n",
      "clo acc: 18.52792076547755\n",
      "spl acc: 19.65772880690549\n",
      "far acc: 34.0518591217395\n",
      "agg cor: (-0.8861779195808417, 7.232834278650833e-178)\n",
      "clo cor: (-0.4560009902923457, 1.7906809857010429e-28)\n",
      "spl cor: (-0.5125048638174236, 1.0609677577036421e-36)\n",
      "far cor: (-0.864174951180753, 5.200680015809707e-159)\n",
      "Evaluating sample #9\n",
      "Initializing featurizers\n",
      "Obtaining training features\n",
      "Evaluating model\n",
      "Got here to composite score\n",
      "Most recent stats:\n",
      "agg acc: 24.046209328341682\n",
      "clo acc: 18.429641808519204\n",
      "spl acc: 19.693467715322047\n",
      "far acc: 34.02026095170745\n",
      "agg cor: (-0.881236884541505, 2.6049799234726472e-173)\n",
      "clo cor: (-0.42574145054982565, 1.1653168048128728e-24)\n",
      "spl cor: (-0.5195249596346924, 7.798792194683714e-38)\n",
      "far cor: (-0.8642056203554739, 4.921872349097669e-159)\n"
     ]
    }
   ],
   "source": [
    "results = evaluate_imaginative_listener_samples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"../results/imaginative_listener_assessment.pkl\", \"wb\") as file:\n",
    "    pickle.dump(results, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.8857654592635387"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([cor[0] for cor in results['aggregate_correlations']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[24.225927799225445]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aggregate_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[18.50006086024096]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "close_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[34.470457996575014]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "far_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[19.710338275301137]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(-0.8799177945947197, 1.4376357425904765e-168)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aggregate_correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(-0.4409533567945598, 5.226923223263904e-26)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "close_correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(-0.8577761621065179, 5.865956224847408e-151)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "far_correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(-0.5079681530415953, 2.896292325793216e-35)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Literal Listener\n",
    "# -----------------------------------------\n",
    "# TODO: FILL IN PARAMETERS \n",
    "def literal_listener_experiment(train=False, epochs=5, embed_dim = 100, hidden_dim = 100, color_dim= 54, model_file=\"../model/literal_listener_5epoch-2.params\"):\n",
    "\n",
    "    # Initializing featurizers\n",
    "    print(\"Initializing featurizers\")\n",
    "    caption_phi = caption_featurizers.CaptionFeaturizer(tokenizer=caption_featurizers.EndingTokenizer) # Use with parameter files that end in `endings_tkn`\n",
    "    # caption_phi = caption_featurizers.CaptionFeaturizer(tokenizer=caption_featurizers.WhitespaceTokenizer) # Use with parameter files don't\n",
    "    color_phi = ColorFeaturizer(color_phi_fourier, \"rgb\", normalized=True)\n",
    "    feature_handler = FeatureHandler(train_data, test_data_synth, caption_phi, color_phi) # target function is initialized by default\n",
    "\n",
    "    print(\"Initializing model\")\n",
    "    model = LiteralListener(CaptionEncoder, num_epochs = epochs)\n",
    "    model.init_model(embed_dim = embed_dim, hidden_dim = hidden_dim, vocab_size = feature_handler.caption_featurizer.caption_indexer.size,\n",
    "                 color_dim = color_dim)\n",
    "\n",
    "    model.load_model(model_file)\n",
    "\n",
    "    # convert the model output to a score for that particular round\n",
    "    print(\"Evaluating model\")\n",
    "    output_to_score = lambda model_outputs, targets: np.exp(model_outputs[np.arange(len(model_outputs)), targets]) # get the model's predicted probablity at each target index and use that as the score\n",
    "    my_score_model = partial(score_model, speaker=Speaker.BY_GAME_ID_COND, return_df=True, score=Score.COMPOSITE)\n",
    "    return evaluate_model(test_data_synth, feature_handler, model, output_to_score, my_score_model, accuracy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I assume pragmatic will either be similar or need its own type of thing\n",
    "def evaluate_literal_listener_samples():\n",
    "    model_directory # FILL THIS IN = \"../literal_listener_samples\"\n",
    "    num_samples = 10\n",
    "    aggregate_correlations = []\n",
    "    close_correlations = []\n",
    "    split_correlations = []\n",
    "    far_correlations = []\n",
    "\n",
    "    aggregate_accuracies = []\n",
    "    close_accuracies = []\n",
    "    split_accuracies = []\n",
    "    far_accuracies = []\n",
    "\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        _, listener_eval = literal_listener(\"{}/sample_{}.params\".format(model_directory, i))\n",
    "        true_scores = listener_eval.groupby('gameid').numOutcome.mean()\n",
    "        model_scores = listener_eval.groupby('gameid').model_scores.mean()\n",
    "        true_scores_composite = composite_score(listener_eval)\n",
    "\n",
    "        aggregate_correlations.append(stats.pearsonr(model_scores, true_scores_composite))\n",
    "        # arbitrarily say we get it right if we assign a majority of the probability mass to it\n",
    "        aggregate_accuracies.append(sum(listener_eval.model_scores > 0.5)/len(listener_eval.model_scores))\n",
    "\n",
    "        # separate out conditions\n",
    "        listener_close = listener_eval[listener_eval.condition == \"close\"]\n",
    "        listener_split = listener_eval[listener_eval.condition == \"split\"]\n",
    "        listener_far =   listener_eval[listener_eval.condition == \"far\"]\n",
    "\n",
    "        listener_close_true_scores =  listener_close.groupby('gameid').numOutcome.mean()\n",
    "        listener_close_model_scores = listener_close.groupby('gameid').model_scores.mean()\n",
    "\n",
    "        listener_split_true_scores =  listener_split.groupby('gameid').numOutcome.mean()\n",
    "        listener_split_model_scores = listener_split.groupby('gameid').model_scores.mean()\n",
    "\n",
    "        listener_far_true_scores =  listener_far.groupby('gameid').numOutcome.mean()\n",
    "        listener_far_model_scores = listener_far.groupby('gameid').model_scores.mean()\n",
    "\n",
    "        # turn true scores to composite, gricean scores\n",
    "        listener_close_composite = composite_score(listener_close)\n",
    "        listener_split_composite = composite_score(listener_split)\n",
    "        listener_far_composite   = composite_score(listener_far)\n",
    "\n",
    "        close_correlations.append(stats.pearsonr(listener_close_composite, listener_close_model_scores))\n",
    "        split_correlations.append(stats.pearsonr(listener_split_composite, listener_split_model_scores))\n",
    "        far_correlations.append(stats.pearsonr(listener_far_composite,     listener_far_model_scores))\n",
    "\n",
    "        close_accuracies.append(sum(listener_close_model_scores > 0.5)/len(listener_close_model_scores))\n",
    "        split_accuracies.append(sum(listener_split_model_scores > 0.5)/len(listener_split_model_scores))\n",
    "        far_accuracies.append(sum(  listener_far_model_scores > 0.5)/len(listener_far_model_scores))\n",
    "\n",
    "    return {\"aggregate_accuracies\": aggregate_accuracies,\n",
    "            \"close_accuracies\": close_accuracies,\n",
    "            \"split_accuracies\": split_accuracies,\n",
    "            \"far_accuracies\": far_accuracies,\n",
    "            \"aggregate_correlations\": aggregate_correlations,\n",
    "            \"close_correlations\":close_correlations,\n",
    "            \"far_correlations\":far_correlations,\n",
    "            \"split_correlations\": split_correlations}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
