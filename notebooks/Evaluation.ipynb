{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, so we can write a class/function that performs the evaluation\n",
    "Given the scores created by the model and the training data, output \n",
    "the summary statistic (which will likely be a correlation coefficient)\n",
    "\n",
    "At the moment, there are three degrees of freedom:\n",
    "1. Whether to use the spearman (ranked) correlation or pearson correlation (default pearson)\n",
    "2. Whether to treat each game as its own speaker or to treat each unique worker as own speaker (default game)\n",
    "3. Whether to correlated with the simple pragmatic metric (average number of correct guesses) or composite metric\n",
    "   (include length of utterance, word specificity, click time, etc.) It might also be interesting to\n",
    "   investigate weighting the different conditions differently, but for now we are doing a macro-average\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so we can use packages from parent directory\n",
    "import sys\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats # for pearsonr, spearmanr\n",
    "import numpy as np\n",
    "import pandas as pd # for handling test data frame\n",
    "from enum import Enum, auto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relevant enums for options\n",
    "class Speaker(Enum):\n",
    "    BY_GAME_ID = \"gameid\"\n",
    "    BY_WORKER_ID = \"workerid_uniq\"\n",
    "    \n",
    "class Regressor(Enum):\n",
    "    PEARSON = stats.pearsonr\n",
    "    SPEARMAN = stats.spearmanr\n",
    "    \n",
    "class Score(Enum):\n",
    "    SIMPLE = auto()\n",
    "    COMPOSITE = auto()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_scores(eval_df, score=Score.SIMPLE):\n",
    "    if score == score.SIMPLE:\n",
    "        return eval_df.groupby(speaker.value).mean()\n",
    "\n",
    "\n",
    "def score_model(test_data, scores, speaker=Speaker.BY_GAME_ID, regressor=Regressor.PEARSON, score=Score.SIMPLE):\n",
    "    \"\"\"\n",
    "    Assume scores are in the same order as the test data (i.e. 0th row is 0th score)\n",
    "    \"\"\"\n",
    "    relevant_columns = [\"gameid\", \"roundNum\", \"numOutcome\"]\n",
    "    if speaker == Speaker.BY_WORKER_ID:\n",
    "        relevant_columns.append(Speaker.BY_WORKER_ID.value)\n",
    "    \n",
    "    if score == Score.COMPOSITE:\n",
    "        # no support for this yet but probably also need:\n",
    "        relevant_columns.extend([\"contents\", \"clkTime\", \"msgTime\"])\n",
    "    \n",
    "    eval_df = test_data.data[relevant_columns].copy()\n",
    "    eval_df[\"model_scores\"] = scores # why we need scores to be in same order as rows\n",
    "    \n",
    "    \n",
    "    if score == score.SIMPLE:\n",
    "        # calculate scores as the mean of the number of successful utterances\n",
    "        # a speaker has\n",
    "        true_scores = eval_df.groupby(speaker.value).numOutcome.mean()\n",
    "    else:\n",
    "        true_scores = calculate_scores(eval_df, score)\n",
    "    \n",
    "    # calculate a model score \n",
    "    model_scores = eval_df.groupby(speaker.value).model_scores.mean()\n",
    "    \n",
    "    result = regressor(true_scores, model_scores)\n",
    "    return result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = []\n",
    "x.append(Speaker.BY_GAME_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Speaker.BY_GAME_ID: 'gameid'>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Speaker.BY_GAME_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4548019047027907\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "hypothesis = ['It', 'is', 'a', 'cat', 'at', 'room']\n",
    "reference = ['It', 'is', 'a', 'cat', 'inside', 'the', 'room']\n",
    "#there may be several references\n",
    "BLEUscore = nltk.translate.bleu_score.sentence_bleu([reference], hypothesis)\n",
    "print(BLEUscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_speaker_bleu(true_captions, generated_captions):\n",
    "    total_bleu = 0\n",
    "    for i in range(len(true_captions)):\n",
    "        reference = [true_captions[0]]\n",
    "        total_bleu += nltk.translate.bleu_score.sentence_bleu(reference, generated_captions[i])\n",
    "    return total_bleu/len(true_captions) # avg bleu\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
