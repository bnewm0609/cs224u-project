{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so we can use packages from parent directory\n",
    "import sys\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColorEncoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, color_dim, hidden_dim):\n",
    "        super(ColorEncoder, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.color_dim = color_dim\n",
    "        self.color_lstm = nn.LSTM(color_dim, hidden_dim, batch_first=True)\n",
    "\n",
    "    def forward(self, colors):\n",
    "        \"\"\"\n",
    "        Colors should be in order with target LAST\n",
    "        \"\"\"\n",
    "        color_states = self.init_hidden_and_context()\n",
    "        color_output, (hn, cn) = self.color_lstm(colors, color_states)\n",
    "        # target is last - return hidden representation, why not context i have no idea\n",
    "        return hn\n",
    "\n",
    "    def init_hidden_and_context(self):\n",
    "        return (torch.zeros(1, 1, self.hidden_dim),\n",
    "                torch.zeros(1, 1, self.hidden_dim))\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Speaker(nn.Module):\n",
    "    \n",
    "    def __init__(self, color_dim, vocab_size, embed_dim, speaker_hidden_dim):\n",
    "        super(Speaker, self).__init__()\n",
    "        \n",
    "        # self.color_encoder = ColorEncoder(color_in_dim, color_dim)\n",
    "        self.embed = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.speaker_lstm = nn.LSTM(embed_dim + color_dim, speaker_hidden_dim, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_dim, vocab_size)\n",
    "        self.logsoftmax = nn.LogSoftmax(dim=2)\n",
    "        \n",
    "        self.hidden_dim = speaker_hidden_dim\n",
    "        \n",
    "    def forward(self, color_features, captions):\n",
    "        # all teacher forcing during training\n",
    "        embeds = self.embed(captions)\n",
    "        #print(\"Embed Shape:\", embeds.shape)\n",
    "        \n",
    "        color_features = color_features.repeat(1, captions.shape[1], 1) # repeat for number of tokens\n",
    "        \n",
    "        inputs = torch.cat((embeds, color_features), dim=2) # cat along the innermost dimension\n",
    "        #print(\"Input Shape:\", inputs.shape)\n",
    "        hiddens, _ = self.speaker_lstm(inputs) # hidden and context default to 0\n",
    "        #print(\"Hiddens Shape:\", hiddens.shape)\n",
    "        outputs = self.linear(hiddens)\n",
    "        #print(\"Outputs Shape:\", outputs.shape)\n",
    "        output_norm = self.logsoftmax(outputs)\n",
    "        return output_norm\n",
    "        \n",
    "    def sample(self, color_features, caption_init):\n",
    "        \n",
    "        for i in range(self.max_gen_len):\n",
    "        vocab_preds = lit_speaker(color_hidden, tokens)[:,-1:,:]\n",
    "        _, prediction_index = vocab_preds.max(2)\n",
    "        tokens = torch.cat((tokens, prediction_index), dim=1)\n",
    "        if prediction_index.item() == end_index:\n",
    "            break\n",
    "        \n",
    "        # caption init should just be a tensor with the start token index\n",
    "        sampled_ids = []\n",
    "        start_embed = self.embed(caption_init)\n",
    "        start_input = torch.cat((start_embed, color_features), dim=2)\n",
    "        context_and_hidden = (torch.zeros(1, 1, self.hidden_dim), torch.zeros(1, 1, self.hidden_dim))\n",
    "        \n",
    "        for i in range(20): # max seq length\n",
    "            output, context_and_hidden = self.speaker_lstm(start_input, context_and_hidden)\n",
    "            output = self.linear(output[-1])\n",
    "            _, predicted_id = output.max(1)\n",
    "            \n",
    "            \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "from monroe_data import MonroeData, MonroeDataEntry, Color\n",
    "from color_featurizers import ColorFeaturizer, color_phi_fourier\n",
    "from caption_featurizers import CaptionFeaturizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_phi = ColorFeaturizer(color_phi_fourier, \"rgb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "monroe_data_train = MonroeData(\"../data/csv/train_corpus_monroe.csv\", \"../data/entries/train_entries_monroe.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_colors_test = color_phi.to_color_features(monroe_data_train[0].colors)\n",
    "train_colors_test = np.flip(train_colors_test, axis=0) # flip bc that's how monroe does it\n",
    "train_colors_test = train_colors_test.copy() # to remove 'negative stride' error\n",
    "train_colors_test_tensor = torch.tensor([train_colors_test]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.0000e+00, -9.9248e-01,  9.7003e-01,  9.9248e-01, -1.0000e+00,\n",
       "           9.9248e-01,  9.7003e-01, -9.9248e-01,  1.0000e+00,  8.7009e-01,\n",
       "          -9.2388e-01,  9.6378e-01,  8.0321e-01, -8.7009e-01,  9.2388e-01,\n",
       "           7.2425e-01, -8.0321e-01,  8.7009e-01,  5.1410e-01, -6.1523e-01,\n",
       "           7.0711e-01,  4.0524e-01, -5.1410e-01,  6.1523e-01,  2.9028e-01,\n",
       "          -4.0524e-01,  5.1410e-01,  0.0000e+00, -1.2241e-01,  2.4298e-01,\n",
       "          -1.2241e-01, -1.2246e-16,  1.2241e-01, -2.4298e-01,  1.2241e-01,\n",
       "           0.0000e+00, -4.9290e-01,  3.8268e-01, -2.6671e-01, -5.9570e-01,\n",
       "           4.9290e-01, -3.8268e-01, -6.8954e-01,  5.9570e-01, -4.9290e-01,\n",
       "          -8.5773e-01,  7.8835e-01, -7.0711e-01, -9.1421e-01,  8.5773e-01,\n",
       "          -7.8835e-01, -9.5694e-01,  9.1421e-01, -8.5773e-01],\n",
       "         [ 1.0000e+00, -9.7832e-01,  9.1421e-01,  9.7832e-01, -1.0000e+00,\n",
       "           9.7832e-01,  9.1421e-01, -9.7832e-01,  1.0000e+00, -5.5557e-01,\n",
       "           3.7132e-01, -1.7096e-01, -7.1573e-01,  5.5557e-01, -3.7132e-01,\n",
       "          -8.4485e-01,  7.1573e-01, -5.5557e-01, -3.8268e-01,  5.6573e-01,\n",
       "          -7.2425e-01, -1.8304e-01,  3.8268e-01, -5.6573e-01,  2.4541e-02,\n",
       "           1.8304e-01, -3.8268e-01,  0.0000e+00, -2.0711e-01,  4.0524e-01,\n",
       "          -2.0711e-01, -1.2246e-16,  2.0711e-01, -4.0524e-01,  2.0711e-01,\n",
       "           0.0000e+00, -8.3147e-01,  9.2851e-01, -9.8528e-01, -6.9838e-01,\n",
       "           8.3147e-01, -9.2851e-01, -5.3500e-01,  6.9838e-01, -8.3147e-01,\n",
       "           9.2388e-01, -8.2459e-01,  6.8954e-01,  9.8311e-01, -9.2388e-01,\n",
       "           8.2459e-01,  9.9970e-01, -9.8311e-01,  9.2388e-01],\n",
       "         [ 1.0000e+00, -9.5694e-01,  8.3147e-01,  6.2486e-01, -8.2459e-01,\n",
       "           9.5331e-01, -2.1910e-01, -7.3565e-02,  3.5990e-01,  9.5694e-01,\n",
       "          -1.0000e+00,  9.5694e-01,  3.7132e-01, -6.2486e-01,  8.2459e-01,\n",
       "          -4.9290e-01,  2.1910e-01,  7.3565e-02,  8.3147e-01, -9.5694e-01,\n",
       "           1.0000e+00,  8.5797e-02, -3.7132e-01,  6.2486e-01, -7.2425e-01,\n",
       "           4.9290e-01, -2.1910e-01,  0.0000e+00, -2.9028e-01,  5.5557e-01,\n",
       "          -7.8074e-01,  5.6573e-01, -3.0201e-01, -9.7570e-01,  9.9729e-01,\n",
       "          -9.3299e-01, -2.9028e-01, -1.2246e-16,  2.9028e-01, -9.2851e-01,\n",
       "           7.8074e-01, -5.6573e-01, -8.7009e-01,  9.7570e-01, -9.9729e-01,\n",
       "          -5.5557e-01,  2.9028e-01,  0.0000e+00, -9.9631e-01,  9.2851e-01,\n",
       "          -7.8074e-01, -6.8954e-01,  8.7009e-01, -9.7570e-01]]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_colors_test_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_dim = 54\n",
    "hidden_dim = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_encoder = ColorEncoder(color_dim, hidden_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_features = color_encoder(train_colors_test_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1658, -0.1022, -0.1696, -0.0378,  0.0271, -0.0811,  0.1505,\n",
       "          -0.0862, -0.0571, -0.0744,  0.0032,  0.0606,  0.0461, -0.0845,\n",
       "          -0.0107,  0.0941,  0.0240, -0.0369,  0.0395, -0.0872,  0.0600,\n",
       "           0.0456, -0.0721, -0.1023,  0.0444, -0.0395,  0.0922,  0.0431,\n",
       "           0.0026, -0.0514, -0.0832, -0.0195, -0.1460, -0.0261, -0.0333,\n",
       "          -0.0032,  0.2056, -0.1198,  0.0398,  0.0455, -0.0086,  0.0075,\n",
       "           0.0859, -0.0773, -0.1994,  0.0910,  0.0444, -0.0058, -0.0405,\n",
       "          -0.0457,  0.1161,  0.0155,  0.0046,  0.0096, -0.1006, -0.0037,\n",
       "          -0.1709,  0.0113,  0.0774,  0.0649, -0.0717, -0.1176, -0.0458,\n",
       "           0.1057, -0.0950,  0.0018,  0.1442,  0.1369, -0.0661,  0.0080,\n",
       "          -0.1698,  0.1032,  0.0675, -0.0959,  0.0378,  0.0423, -0.0997,\n",
       "          -0.0509, -0.0090, -0.0861,  0.0369,  0.0457, -0.1105,  0.0781,\n",
       "           0.1225, -0.0491,  0.0774, -0.0174,  0.0240,  0.1001,  0.0759,\n",
       "          -0.0863, -0.1642, -0.1159,  0.0334, -0.0595, -0.0845, -0.1300,\n",
       "           0.2406,  0.1316]]], grad_fn=<StackBackward>)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "color_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_featurs_rpt = color_features.repeat(1, 6, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_embed = nn.Embedding(20, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_caption_embed = test_embed(train_caption_test_tensor) # batch size x seq_len x embed dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_joint_embed = torch.cat((color_featurs_rpt, test_caption_embed), dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6, 200])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_joint_embed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 7, 100])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_joint_embed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 1, 100])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test_embed(train_caption_test_tensor_v2).shape # not when batch_first is true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'caption_featurizers' from '../caption_featurizers.py'>"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "import caption_featurizers\n",
    "importlib.reload(caption_featurizers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "caption_phi = CaptionFeaturizer()\n",
    "caption_phi.construct_featurizer(monroe_data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The darker blue one'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monroe_data_train[0].caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, train_caption_test = caption_phi.to_string_features(monroe_data_train[0].caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_caption_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_caption_test_tensor = torch.tensor([train_caption_test], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2, 3, 4, 5]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_caption_test_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_caption_test_tensor_v2 = train_caption_test_tensor.view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "caption_phi.caption_indexer.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 100\n",
    "color_embed_dim = 100\n",
    "lit_speaker = Speaker(color_embed_dim, caption_phi.caption_indexer.size, embed_dim, hidden_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embed Shape: torch.Size([1, 6, 100])\n",
      "Input Shape: torch.Size([1, 6, 200])\n",
      "Hiddens Shape: torch.Size([1, 6, 100])\n",
      "Outputs Shape: torch.Size([1, 6, 1026])\n"
     ]
    }
   ],
   "source": [
    "test_output = lit_speaker(color_features, train_caption_test_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_output = torch.tensor([[[ 0.0277, -0.1126,  0.1665, -0.0175, -0.0271,  0.1332],\n",
    "         [-0.1007, -0.0795,  0.1356, -0.0220, -0.0995,  0.0179],\n",
    "         [-0.0856, -0.0679,  0.1927, -0.0224, -0.0225,  0.0786],\n",
    "         [ 0.0892, -0.1054,  0.1166,  0.1385,  0.0678, -0.0863],\n",
    "         [ 0.0178, -0.1882,  0.1608,  0.0925, -0.0561,  0.0705],\n",
    "         [ 0.0131, -0.2336,  0.0719, -0.0714, -0.0005,  0.0543]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_output = nn.functional.softmax(test_output, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6, 1026])"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 1026])"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_output[:,:-1,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0000124220102928"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(np.exp([-1.8621, -1.7997, -1.8290, -1.8715, -1.6569, -1.7480]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999999999999999"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.1721+0.1513+0.1536+0.1830+0.1704+0.1696"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([0.1658, 0.1441, 0.1905, 0.1585, 0.1569, 0.1842])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0207"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([0.1721, 0.1695, 0.1709, 0.1607, 0.1658, 0.1817])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2) Game: 1124-1 Round: 3\n",
      "Medium pink ~ the medium dark one\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWQAAADuCAYAAAAOR30qAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAA+hJREFUeJzt201qFFEYhtFbnaxAyMSJmzAobiHr\ndQuimE1k4sSfJdR14ALSJRb3MZwz/gYvNDxcCnqbcw4A1rusHgDAH4IMECHIABGCDBAhyAARggwQ\nIcgAEYIMECHIABG3R45vLjdzn/tZWzjRZWxjH/6V+b/xi70M2xg/5px3z90dCvI+9/H53ae/X8Uy\n7798GD9ffVs9g6N+vV69gH/j6ZojnywAIg69kIH1Pr79unoCV3p4vD9074UMECHIABGCDBAhyAAR\nggwQIcgAEYIMECHIABGCDBAhyAARggwQIcgAEYIMECHIABGCDBAhyAARggwQIcgAEYIMECHIABGC\nDBAhyAARggwQIcgAEYIMECHIABGCDBAhyAARggwQIcgAEYIMECHIABGCDBAhyAARggwQIcgAEYIM\nECHIABGCDBAhyAARggwQIcgAEYIMECHIABGCDBAhyAARggwQIcgAEYIMECHIABGCDBAhyAARggwQ\nIcgAEYIMECHIABGCDBAhyAARggwQIcgAEYIMECHIABGCDBAhyAARggwQIcgAEYIMECHIABGCDBAh\nyAARggwQIcgAEYIMECHIABGCDBAhyAARggwQIcgAEYIMECHIABGCDBAhyAARggwQIcgAEYIMECHI\nABGCDBAhyAARggwQIcgAEYIMECHIABGCDBAhyAARggwQIcgAEYIMECHIABGCDBAhyAARggwQIcgA\nEYIMECHIABGCDBAhyAARggwQIcgAEYIMECHIABGCDBAhyAARggwQIcgAEYIMECHIABGCDBAhyAAR\nggwQIcgAEYIMECHIABGCDBAhyAARggwQIcgAEYIMECHIABGCDBAhyAARggwQIcgAEYIMECHIABGC\nDBAhyAARggwQIcgAEYIMECHIABGCDBAhyAARggwQIcgAEYIMECHIABGCDBAhyAARggwQIcgAEYIM\nECHIABGCDBAhyAARggwQIcgAEYIMECHIABGCDBAhyAARggwQIcgAEYIMECHIABGCDBAhyAARggwQ\nIcgAEYIMECHIABGCDBAhyAARggwQIcgAEYIMECHIABGCDBAhyAARggwQIcgAEYIMECHIABGCDBAh\nyAARggwQIcgAEYIMECHIABGCDBAhyAARggwQIcgAEYIMECHIABGCDBAhyAARggwQIcgAEYIMECHI\nABGCDBAhyAARggwQIcgAEYIMECHIABGCDBAhyAARggwQIcgAEYIMEHG7egBwzMPj/eoJnMQLGSBi\nm3Nef7xt38cYT+fNAXiR3sw57547OhRkAM7jkwVAhCADRAgyQIQgA0QIMkCEIANECDJAhCADRAgy\nQMRv4Z8o7qd8f6AAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "monroe_data_train.display_game(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caption_phi = CaptionFeaturizer()\n",
    "caption_phi.construct_featurizer(monroe_data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data_entry in monroe_data_train:\n",
    "    caption_features = caption_phi.to_string_features(data_entry.caption)\n",
    "    color_features = color_phi.to_color_features(data_entry.colors)\n",
    "    # turn them into torch tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5])"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_caption_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = train_caption_test[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets_tensor = torch.tensor(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3, 4, 5])"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 100])"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "color_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1658, -0.1022, -0.1696, -0.0378,  0.0271, -0.0811,  0.1505,\n",
       "          -0.0862, -0.0571, -0.0744,  0.0032,  0.0606,  0.0461, -0.0845,\n",
       "          -0.0107,  0.0941,  0.0240, -0.0369,  0.0395, -0.0872,  0.0600,\n",
       "           0.0456, -0.0721, -0.1023,  0.0444, -0.0395,  0.0922,  0.0431,\n",
       "           0.0026, -0.0514, -0.0832, -0.0195, -0.1460, -0.0261, -0.0333,\n",
       "          -0.0032,  0.2056, -0.1198,  0.0398,  0.0455, -0.0086,  0.0075,\n",
       "           0.0859, -0.0773, -0.1994,  0.0910,  0.0444, -0.0058, -0.0405,\n",
       "          -0.0457,  0.1161,  0.0155,  0.0046,  0.0096, -0.1006, -0.0037,\n",
       "          -0.1709,  0.0113,  0.0774,  0.0649, -0.0717, -0.1176, -0.0458,\n",
       "           0.1057, -0.0950,  0.0018,  0.1442,  0.1369, -0.0661,  0.0080,\n",
       "          -0.1698,  0.1032,  0.0675, -0.0959,  0.0378,  0.0423, -0.0997,\n",
       "          -0.0509, -0.0090, -0.0861,  0.0369,  0.0457, -0.1105,  0.0781,\n",
       "           0.1225, -0.0491,  0.0774, -0.0174,  0.0240,  0.1001,  0.0759,\n",
       "          -0.0863, -0.1642, -0.1159,  0.0334, -0.0595, -0.0845, -0.1300,\n",
       "           0.2406,  0.1316]]], grad_fn=<StackBackward>)"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "color_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embed Shape: torch.Size([1, 6, 100])\n",
      "Input Shape: torch.Size([1, 6, 200])\n",
      "Hiddens Shape: torch.Size([1, 6, 100])\n",
      "Outputs Shape: torch.Size([1, 6, 1026])\n"
     ]
    }
   ],
   "source": [
    "predictions = lit_speaker(color_features, train_caption_test_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predictions[:,:-1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 1026])"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_squeeze = predictions.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-6.9851, -6.9951, -7.0918,  ..., -7.0591, -6.9044, -6.9136],\n",
       "        [-6.9592, -7.0131, -7.0628,  ..., -6.9723, -6.8610, -6.9665],\n",
       "        [-7.0227, -6.9164, -6.9528,  ..., -7.0429, -6.9906, -6.7736],\n",
       "        [-7.1074, -6.9282, -7.0202,  ..., -7.0575, -6.9063, -6.7328],\n",
       "        [-7.0327, -6.9017, -6.9920,  ..., -7.1274, -7.0435, -6.7871]],\n",
       "       grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_squeeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-6.9851, -6.9951, -7.0918,  ..., -7.0591, -6.9044, -6.9136],\n",
       "         [-6.9592, -7.0131, -7.0628,  ..., -6.9723, -6.8610, -6.9665],\n",
       "         [-7.0227, -6.9164, -6.9528,  ..., -7.0429, -6.9906, -6.7736],\n",
       "         [-7.1074, -6.9282, -7.0202,  ..., -7.0575, -6.9063, -6.7328],\n",
       "         [-7.0327, -6.9017, -6.9920,  ..., -7.1274, -7.0435, -6.7871]]],\n",
       "       grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7.0452, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_criterion(predictions_squeeze, targets_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_optimer = torch.optim.Adam(lr=0.004, params=lit_speaker.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embed Shape: torch.Size([1, 6, 100])\n",
      "Input Shape: torch.Size([1, 6, 200])\n",
      "Hiddens Shape: torch.Size([1, 6, 100])\n",
      "Outputs Shape: torch.Size([1, 6, 1026])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(7.0452, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_iter(color_encoder, lit_speaker, train_caption_test_tensor, train_colors_test_tensor, targets_tensor,\n",
    "          test_optimer, test_criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = []\n",
    "targets = []\n",
    "for data_entry in monroe_data_train:\n",
    "    _, caption_features = caption_phi.to_string_features(data_entry.caption)\n",
    "    color_features = color_phi.to_color_features(data_entry.colors)\n",
    "    # reverse color order\n",
    "    color_features = np.flip(color_features, axis=0).copy()\n",
    "    features.append((caption_features, color_features))\n",
    "    targets.append(caption_features[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2, 3, 4, 5]),\n",
       " array([[ 1.0000000e+00, -9.9247956e-01,  9.7003126e-01,  9.9247956e-01,\n",
       "         -1.0000000e+00,  9.9247956e-01,  9.7003126e-01, -9.9247956e-01,\n",
       "          1.0000000e+00,  8.7008697e-01, -9.2387950e-01,  9.6377605e-01,\n",
       "          8.0320752e-01, -8.7008697e-01,  9.2387950e-01,  7.2424710e-01,\n",
       "         -8.0320752e-01,  8.7008697e-01,  5.1410276e-01, -6.1523157e-01,\n",
       "          7.0710677e-01,  4.0524131e-01, -5.1410276e-01,  6.1523157e-01,\n",
       "          2.9028466e-01, -4.0524131e-01,  5.1410276e-01,  0.0000000e+00,\n",
       "         -1.2241068e-01,  2.4298018e-01, -1.2241068e-01, -1.2246469e-16,\n",
       "          1.2241068e-01, -2.4298018e-01,  1.2241068e-01,  0.0000000e+00,\n",
       "         -4.9289820e-01,  3.8268343e-01, -2.6671275e-01, -5.9569931e-01,\n",
       "          4.9289820e-01, -3.8268343e-01, -6.8954057e-01,  5.9569931e-01,\n",
       "         -4.9289820e-01, -8.5772860e-01,  7.8834641e-01, -7.0710677e-01,\n",
       "         -9.1420978e-01,  8.5772860e-01, -7.8834641e-01, -9.5694035e-01,\n",
       "          9.1420978e-01, -8.5772860e-01],\n",
       "        [ 1.0000000e+00, -9.7831738e-01,  9.1420978e-01,  9.7831738e-01,\n",
       "         -1.0000000e+00,  9.7831738e-01,  9.1420978e-01, -9.7831738e-01,\n",
       "          1.0000000e+00, -5.5557024e-01,  3.7131721e-01, -1.7096189e-01,\n",
       "         -7.1573085e-01,  5.5557024e-01, -3.7131721e-01, -8.4485358e-01,\n",
       "          7.1573085e-01, -5.5557024e-01, -3.8268343e-01,  5.6573182e-01,\n",
       "         -7.2424710e-01, -1.8303989e-01,  3.8268343e-01, -5.6573182e-01,\n",
       "          2.4541229e-02,  1.8303989e-01, -3.8268343e-01,  0.0000000e+00,\n",
       "         -2.0711137e-01,  4.0524131e-01, -2.0711137e-01, -1.2246469e-16,\n",
       "          2.0711137e-01, -4.0524131e-01,  2.0711137e-01,  0.0000000e+00,\n",
       "         -8.3146960e-01,  9.2850608e-01, -9.8527765e-01, -6.9837624e-01,\n",
       "          8.3146960e-01, -9.2850608e-01, -5.3499764e-01,  6.9837624e-01,\n",
       "         -8.3146960e-01,  9.2387950e-01, -8.2458931e-01,  6.8954057e-01,\n",
       "          9.8310548e-01, -9.2387950e-01,  8.2458931e-01,  9.9969882e-01,\n",
       "         -9.8310548e-01,  9.2387950e-01],\n",
       "        [ 1.0000000e+00, -9.5694035e-01,  8.3146960e-01,  6.2485951e-01,\n",
       "         -8.2458931e-01,  9.5330602e-01, -2.1910124e-01, -7.3564567e-02,\n",
       "          3.5989505e-01,  9.5694035e-01, -1.0000000e+00,  9.5694035e-01,\n",
       "          3.7131721e-01, -6.2485951e-01,  8.2458931e-01, -4.9289820e-01,\n",
       "          2.1910124e-01,  7.3564567e-02,  8.3146960e-01, -9.5694035e-01,\n",
       "          1.0000000e+00,  8.5797310e-02, -3.7131721e-01,  6.2485951e-01,\n",
       "         -7.2424710e-01,  4.9289820e-01, -2.1910124e-01,  0.0000000e+00,\n",
       "         -2.9028466e-01,  5.5557024e-01, -7.8073722e-01,  5.6573182e-01,\n",
       "         -3.0200595e-01, -9.7570211e-01,  9.9729043e-01, -9.3299282e-01,\n",
       "         -2.9028466e-01, -1.2246469e-16,  2.9028466e-01, -9.2850608e-01,\n",
       "          7.8073722e-01, -5.6573182e-01, -8.7008697e-01,  9.7570211e-01,\n",
       "         -9.9729043e-01, -5.5557024e-01,  2.9028466e-01,  0.0000000e+00,\n",
       "         -9.9631262e-01,  9.2850608e-01, -7.8073722e-01, -6.8954057e-01,\n",
       "          8.7008697e-01, -9.7570211e-01]], dtype=float32))"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_iter(color_encoder, caption_generator, caption_tensor, color_tensor, target, optimizer, criterion):\n",
    "    \"\"\"\n",
    "    Iterates through a single training pair, querying the model, getting a loss and\n",
    "    updating the parameters. (TODO: addd some kind of batching to this).\n",
    "\n",
    "    Very much inspired by the torch NMT example/tutorial thingy\n",
    "    \"\"\"\n",
    "    # start_states = self.model.init_hidden_and_context()\n",
    "    #input_length = caption_tensor.size(0)\n",
    "    optimizer.zero_grad()\n",
    "    loss = 0\n",
    "\n",
    "    color_features = color_encoder(color_tensor)\n",
    "    model_output = caption_generator(color_features, caption_tensor)\n",
    "\n",
    "    # we don't care about the last prediction, because nothing follows the final </s> token\n",
    "    model_output = model_output[:,:-1,:].squeeze(0) # go from 1 x seq_len x vocab_size => seq_len x vocab_size\n",
    "                                                    # for calculating loss function:\n",
    "                                                    # see here for details when implementing batching\n",
    "                                                    # https://discuss.pytorch.org/t/calculating-loss-for-entire-batch-using-nllloss-in-0-4-0/17142/7\n",
    "\n",
    "    # targets should be caption without start index: i.e. [the blue one </s>] so we can predict\n",
    "    # next tokens from input like [<s> the blue one]\n",
    "\n",
    "    loss += criterion(model_output, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(features, targets, optimizer, criterion, num_epochs, color_encoder, caption_generator, lr=0.004):\n",
    "\n",
    "\n",
    "    optimizer = optimizer(lr=lr, params=list(color_encoder.parameters()) + list(caption_generator.parameters()))\n",
    "    criterion = criterion()\n",
    "    start_time = time.time()\n",
    "    store_losses_every = 100\n",
    "    print_losses_every = 1000\n",
    "    stored_losses = [] # theoretically we store losses so we can plot them later - \n",
    "                            # I don't think this part of the code works though. What we\n",
    "                            # can do instead is take a few thousand or so training examples\n",
    "                            # out and use them for \"evaluation\" every 1 epoch or so\n",
    "    for epoch in range(num_epochs):\n",
    "        print(\"---EPOCH {}---\".format(epoch))\n",
    "        stored_loss_total = 0\n",
    "        print_loss_total = 0\n",
    "\n",
    "        for i, pair in enumerate(features):\n",
    "            caption, colors = pair\n",
    "            caption = torch.tensor([caption], dtype=torch.long)\n",
    "            colors = torch.tensor([colors], dtype=torch.float)\n",
    "            # this was changed\n",
    "            target = torch.tensor(targets[i]) # already turned it into a tensor in `self.fit`\n",
    "\n",
    "\n",
    "            loss = train_iter(color_encoder, caption_generator, caption, colors, target, optimizer, criterion)\n",
    "            stored_loss_total += loss.item()\n",
    "            print_loss_total += loss.item()\n",
    "\n",
    "            if i % print_losses_every == 0:\n",
    "                print_loss_avg = print_loss_total / print_losses_every\n",
    "                print(\"{} ({}:{} {:.2f}%) {:.4f}\".format(asMinutes(time.time() - start_time),\n",
    "                                                  epoch, i, i/len(features)*100,\n",
    "                                                  print_loss_avg))\n",
    "                print_loss_total = 0\n",
    "\n",
    "            if i % store_losses_every == 0:\n",
    "                stored_loss_avg = stored_loss_total / store_losses_every\n",
    "                stored_losses.append(stored_loss_avg)\n",
    "                stored_loss_total = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---EPOCH 0---\n",
      "0m 0s (0:0 0.00%) 0.0069\n",
      "0m 5s (0:1000 6.38%) 3.0628\n",
      "0m 12s (0:2000 12.77%) 3.0255\n",
      "0m 19s (0:3000 19.15%) 3.2140\n",
      "0m 26s (0:4000 25.53%) 3.0905\n",
      "0m 34s (0:5000 31.92%) 3.3288\n",
      "0m 42s (0:6000 38.30%) 2.8381\n",
      "0m 50s (0:7000 44.69%) 2.5258\n",
      "0m 59s (0:8000 51.07%) 2.8705\n",
      "1m 8s (0:9000 57.45%) 2.3008\n",
      "1m 17s (0:10000 63.84%) 2.4063\n",
      "1m 26s (0:11000 70.22%) 2.1981\n",
      "1m 35s (0:12000 76.60%) 2.2217\n",
      "1m 44s (0:13000 82.99%) 2.4715\n",
      "1m 54s (0:14000 89.37%) 2.4886\n",
      "2m 3s (0:15000 95.75%) 2.8288\n",
      "---EPOCH 1---\n",
      "2m 10s (1:0 0.00%) 0.0015\n",
      "2m 19s (1:1000 6.38%) 2.2457\n",
      "2m 29s (1:2000 12.77%) 2.6647\n",
      "2m 39s (1:3000 19.15%) 2.8676\n",
      "2m 49s (1:4000 25.53%) 2.8568\n",
      "2m 59s (1:5000 31.92%) 3.1168\n",
      "3m 9s (1:6000 38.30%) 2.5957\n",
      "3m 19s (1:7000 44.69%) 2.2874\n",
      "3m 29s (1:8000 51.07%) 2.6781\n",
      "3m 40s (1:9000 57.45%) 2.1543\n",
      "3m 50s (1:10000 63.84%) 2.2300\n",
      "4m 0s (1:11000 70.22%) 2.0911\n",
      "4m 11s (1:12000 76.60%) 2.0938\n",
      "4m 22s (1:13000 82.99%) 2.3372\n",
      "4m 33s (1:14000 89.37%) 2.3387\n",
      "4m 44s (1:15000 95.75%) 2.6998\n",
      "---EPOCH 2---\n",
      "4m 51s (2:0 0.00%) 0.0016\n",
      "5m 2s (2:1000 6.38%) 2.1613\n",
      "5m 13s (2:2000 12.77%) 2.5941\n",
      "5m 24s (2:3000 19.15%) 2.8224\n",
      "5m 35s (2:4000 25.53%) 2.8294\n",
      "5m 46s (2:5000 31.92%) 3.0241\n",
      "5m 58s (2:6000 38.30%) 2.5318\n",
      "6m 9s (2:7000 44.69%) 2.2623\n",
      "6m 20s (2:8000 51.07%) 2.6288\n",
      "6m 31s (2:9000 57.45%) 2.1251\n",
      "6m 42s (2:10000 63.84%) 2.1962\n",
      "6m 54s (2:11000 70.22%) 2.0674\n",
      "7m 5s (2:12000 76.60%) 2.0632\n",
      "7m 17s (2:13000 82.99%) 2.2918\n",
      "7m 28s (2:14000 89.37%) 2.3199\n",
      "7m 40s (2:15000 95.75%) 2.6815\n",
      "---EPOCH 3---\n",
      "7m 48s (3:0 0.00%) 0.0019\n",
      "8m 0s (3:1000 6.38%) 2.1483\n",
      "8m 12s (3:2000 12.77%) 2.5609\n",
      "8m 24s (3:3000 19.15%) 2.7545\n",
      "8m 36s (3:4000 25.53%) 2.7863\n",
      "8m 48s (3:5000 31.92%) 2.9874\n",
      "9m 0s (3:6000 38.30%) 2.5257\n",
      "9m 12s (3:7000 44.69%) 2.2562\n",
      "9m 24s (3:8000 51.07%) 2.6290\n",
      "9m 37s (3:9000 57.45%) 2.1338\n",
      "9m 49s (3:10000 63.84%) 2.1637\n",
      "10m 1s (3:11000 70.22%) 2.0503\n",
      "10m 14s (3:12000 76.60%) 2.0394\n",
      "10m 26s (3:13000 82.99%) 2.2951\n",
      "10m 39s (3:14000 89.37%) 2.3269\n",
      "10m 52s (3:15000 95.75%) 2.6687\n",
      "---EPOCH 4---\n",
      "11m 1s (4:0 0.00%) 0.0018\n",
      "11m 14s (4:1000 6.38%) 2.1123\n",
      "11m 27s (4:2000 12.77%) 2.5315\n",
      "11m 41s (4:3000 19.15%) 2.8170\n",
      "11m 54s (4:4000 25.53%) 2.8159\n",
      "12m 7s (4:5000 31.92%) 2.9921\n",
      "12m 21s (4:6000 38.30%) 2.5455\n",
      "12m 34s (4:7000 44.69%) 2.2583\n",
      "12m 47s (4:8000 51.07%) 2.6371\n",
      "13m 1s (4:9000 57.45%) 2.1252\n",
      "13m 14s (4:10000 63.84%) 2.1547\n",
      "13m 27s (4:11000 70.22%) 2.0562\n",
      "13m 40s (4:12000 76.60%) 2.0284\n",
      "13m 54s (4:13000 82.99%) 2.2835\n",
      "14m 7s (4:14000 89.37%) 2.2915\n",
      "14m 21s (4:15000 95.75%) 2.6706\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam\n",
    "criterion = nn.NLLLoss\n",
    "epochs = 5\n",
    "\n",
    "train_model(features, targets, optimizer, criterion, epochs, color_encoder, lit_speaker, lr=0.004)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, start_end_tokens = caption_phi.to_string_features(\"\")\n",
    "start_token = start_end_tokens[:1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_token_tensor = torch.tensor([start_token])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0]])"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_token_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_token_tensor.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_features = color_phi.to_color_features(monroe_data_train[0].colors)\n",
    "color_features = np.flip(color_features).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_features_tensor = torch.tensor([color_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 54])"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "color_features_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    color_hidden = color_encoder(color_features_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-5.8963e-11,  1.4735e-03,  1.3294e-08, -1.4023e-09,  8.2325e-11,\n",
       "           3.3685e-17,  1.2891e-01, -1.3805e-02, -1.7243e-05, -7.6492e-01,\n",
       "          -2.4686e-04,  5.3317e-08,  6.1667e-03, -9.9964e-05, -6.9311e-01,\n",
       "          -1.5405e-10, -4.1267e-12,  4.0509e-01,  6.9762e-18, -6.5709e-11,\n",
       "          -4.6384e-15,  9.9269e-01,  1.8415e-02,  4.9983e-05,  2.0152e-02,\n",
       "           9.6281e-01,  5.2528e-09,  1.6348e-10,  1.3820e-01,  2.6305e-01,\n",
       "          -7.5883e-01, -7.3660e-01, -1.2099e-03, -6.4656e-06, -3.9381e-01,\n",
       "           2.7583e-05,  1.9473e-05,  1.0987e-02,  4.3957e-01, -7.6060e-01,\n",
       "          -4.3767e-10,  4.8078e-07, -6.0415e-01, -7.5754e-01, -3.5695e-05,\n",
       "           5.5961e-01, -7.8295e-08,  5.9885e-17, -1.9891e-03, -4.3265e-08,\n",
       "          -2.9429e-05, -1.3059e-07,  2.3924e-10,  1.2350e-07,  5.4578e-07,\n",
       "          -7.6108e-01,  9.5138e-04,  1.9278e-04,  5.6679e-10, -4.0206e-01,\n",
       "          -5.6103e-07,  2.8893e-07,  1.2813e-01, -7.6079e-01,  2.9734e-05,\n",
       "          -7.6151e-10, -4.8507e-05,  4.4402e-03,  2.0050e-01, -3.1941e-04,\n",
       "          -7.3879e-01, -1.3498e-04,  2.3227e-01,  1.6776e-01, -2.3861e-04,\n",
       "           8.7624e-04,  2.6581e-01, -1.1757e-10, -9.4983e-01, -5.3375e-01,\n",
       "          -7.5778e-01, -2.7704e-09, -1.6528e-03, -1.4938e-07,  5.0557e-01,\n",
       "          -7.7919e-04,  1.2340e-02,  8.6364e-09,  6.0388e-08,  5.4389e-02,\n",
       "          -4.8897e-02,  7.5558e-01, -7.0653e-01, -1.4353e-01,  3.8925e-01,\n",
       "          -6.3000e-16, -6.2243e-06,  7.5653e-01,  6.5170e-01, -7.2206e-03]]])"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "color_hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    vocab_preds = lit_speaker(color_hidden, start_token_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 1026])"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, prediction_index = vocab_preds.max(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1]])"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = start_token_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = torch.cat((tokens, prediction_index), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1]])"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    vocab_preds = lit_speaker(color_hidden, tokens)[:,-1:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, prediction_index = vocab_preds.max(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4]])"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-1.8698, -2.1367]]), tensor([[1, 4]]))"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_preds.max(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = torch.cat((tokens, prediction_index), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[:,-1:].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, start_end_tokens = caption_phi.to_string_features(\"\")\n",
    "start_token = start_end_tokens[:1]\n",
    "tokens = torch.tensor([start_token])\n",
    "\n",
    "with torch.no_grad():\n",
    "    color_hidden = color_encoder(color_features_tensor)\n",
    "    \n",
    "with torch.no_grad():\n",
    "    for i in range(self.max_gen_len):\n",
    "        vocab_preds = lit_speaker(color_hidden, tokens)[:,-1:,:]\n",
    "        _, prediction_index = vocab_preds.max(2)\n",
    "        tokens = torch.cat((tokens, prediction_index), dim=1)\n",
    "        if prediction_index.item() == end_index:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12980"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax([len(de.tokens) for de in monroe_data_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12980) Game: 6089-c Round: 3\n",
      "yes, caca green, brown caca, barney? ~ thank you. i take it your a bread on a certain forum ~ ahhhh. bummer, i saw someone talk about having a great partner on a forum ~ let's see how we can get through this. then hopefully we will meet again ~ thank you. you were awesome too ~ grey\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWQAAADuCAYAAAAOR30qAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAA9hJREFUeJzt27FNA0EQQNE7RAdATC1QA9UgE2FR\nDTVALY6BGpaAAvBZtu5j3ovnpEnua7XSzmOMCYD1Xay9AAA/BBkgQpABIgQZIEKQASIEGSBCkAEi\nBBkgQpABIi4XTc+zZ31nYF57ARabp2ny8/1pn2OMm9+GlgWZs/B297j2Cix0//4yfT5crb0GB7p+\n/drtM+fKAiDi4BPy9mlzzD04kc3zdu0VgD05IQNECDJAhCADRAgyQIQgA0QIMkCEIANECDJAhCAD\nRAgyQIQgA0QIMkCEIANECDJAhCADRAgyQIQgA0QIMkCEIANECDJAhCADRAgyQIQgA0QIMkCEIANE\nCDJAhCADRAgyQIQgA0QIMkCEIANECDJAhCADRAgyQIQgA0QIMkCEIANECDJAhCADRAgyQIQgA0QI\nMkCEIANECDJAhCADRAgyQIQgA0QIMkCEIANECDJAhCADRAgyQIQgA0QIMkCEIANECDJAhCADRAgy\nQIQgA0QIMkCEIANECDJAhCADRAgyQIQgA0QIMkCEIANECDJAhCADRAgyQIQgA0QIMkCEIANECDJA\nhCADRAgyQIQgA0QIMkCEIANECDJAhCADRAgyQIQgA0QIMkCEIANECDJAhCADRAgyQIQgA0QIMkCE\nIANECDJAhCADRAgyQIQgA0QIMkCEIANECDJAhCADRAgyQIQgA0QIMkCEIANECDJAhCADRAgyQIQg\nA0QIMkCEIANECDJAhCADRAgyQIQgA0QIMkCEIANECDJAhCADRAgyQIQgA0QIMkCEIANECDJAhCAD\nRAgyQIQgA0QIMkCEIANECDJAhCADRAgyQIQgA0QIMkCEIANECDJAhCADRAgyQIQgA0QIMkCEIANE\nCDJAhCADRAgyQIQgA0QIMkCEIANECDJAhCADRAgyQIQgA0QIMkCEIANECDJAhCADRAgyQIQgA0QI\nMkCEIANECDJAhCADRAgyQIQgA0QIMkCEIANECDJAhCADRAgyQIQgA0QIMkCEIANECDJAhCADRAgy\nQIQgA0QIMkCEIANECDJAhCADRAgyQIQgA0QIMkCEIANECDJAhCADRAgyQIQgA0QIMkCEIANECDJA\nhCADRAgyQIQgA0QIMkCEIANECDJAhCADRAgyQIQgA0QIMkCEIANECDJAhCADRAgyQIQgA0QIMkCE\nIANECDJAhCADRAgyQIQgA0QIMkCEIANEXB764eZ5e8w9AP49J2SAiHmMsf/wPH9M07Q73ToAZ+l2\njHHz29CiIANwOq4sACIEGSBCkAEiBBkgQpABIgQZIEKQASIEGSBCkAEivgFeaiE92xK8OQAAAABJ\nRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "monroe_data_train.display_game(12980)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5]])"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<sos>'"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "caption_phi.caption_indexer.get_word_from_idx(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the'"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "caption_phi.caption_indexer.get_word_from_idx(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'one'"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "caption_phi.caption_indexer.get_word_from_idx(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<eos>'"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "caption_phi.caption_indexer.get_word_from_idx(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LiteralSpeaker():\n",
    "\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.features = X\n",
    "        self.targets = torch.tensor([y])\n",
    "        self.train_model()\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Produces and tracks model outputs\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        model_outputs = np.empty([len(X), 3])\n",
    "\n",
    "        for i, feature in enumerate(X):\n",
    "            caption, colors = feature\n",
    "            caption = torch.tensor(caption, dtype=torch.long).view(-1, 1)\n",
    "            colors = torch.tensor(colors, dtype=torch.float)\n",
    "            model_output_np = self.evaluate_iter((caption, colors)).view(-1).numpy()\n",
    "            model_outputs[i] = model_output_np\n",
    "\n",
    "        return np.array(model_outputs)\n",
    "\n",
    "\n",
    "    def __init__(self,  model, optimizer=torch.optim.Adadelta,\n",
    "                 criterion=nn.NLLLoss, lr=0.2, num_epochs=30):\n",
    "        \"\"\"\n",
    "        Right now this is kind of ugly because you have to pass literally all of the experiment arguments\n",
    "        to this constructor. \n",
    "        \"\"\"\n",
    "\n",
    "        self.model = model\n",
    "\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = criterion\n",
    "\n",
    "        # misc args:\n",
    "        self.lr = lr\n",
    "        self.num_epochs = num_epochs\n",
    "\n",
    "        # for reproducibility, store training pairs\n",
    "        self.features  = None\n",
    "        self.target = None\n",
    "\n",
    "        # also make sure the model has been initialized before we do anything\n",
    "        self.initialized = False\n",
    "\n",
    "\n",
    "    # from https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\n",
    "    def asMinutes(self, s):\n",
    "        m = math.floor(s / 60)\n",
    "        s -= m * 60\n",
    "        return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "    def init_model(self, **model_params):\n",
    "        \"\"\"\n",
    "        Interesting quirk - in most cases, the model takes the vocabulary size\n",
    "        as an input, but there's no way for the user to know what the vocab\n",
    "        size is before calling the init_model method. Even though it's kinda ugly,\n",
    "        what we can do instead is pass all of the model params (named) minus\n",
    "        the vocab size to this function, and it will create the model. I don't \n",
    "        really like this but it works for now\n",
    "        \"\"\"\n",
    "        if self.initialized:\n",
    "            return\n",
    "\n",
    "        # self.caption_featurizer.construct_featurizer(self.train_data)\n",
    "        # self.train_pairs = self.get_pairs(self.train_data, construct=True)\n",
    "        # model_params['vocab_size'] = self.caption_featurizer.caption_indexer.size\n",
    "        self.model = self.model(**model_params)\n",
    "        self.initialized = True\n",
    "\n",
    "    def train_iter(self, caption_tensor, color_tensor, target, optimizer, criterion):\n",
    "        \"\"\"\n",
    "        Iterates through a single training pair, querying the model, getting a loss and\n",
    "        updating the parameters. (TODO: addd some kind of batching to this).\n",
    "\n",
    "        Very much inspired by the torch NMT example/tutorial thingy\n",
    "        \"\"\"\n",
    "        # start_states = self.model.init_hidden_and_context()\n",
    "        #input_length = caption_tensor.size(0)\n",
    "        optimizer.zero_grad()\n",
    "        loss = 0\n",
    "        \n",
    "        color_features = self.color_encoder(color_tensor)\n",
    "        model_output = self.caption_generator(color_features, caption_tensor)\n",
    "        \n",
    "        # we don't care about the last prediction, because nothing follows the final </s> token\n",
    "        model_output = model_output[:,:-1,:].squeeze(0) # go from 1 x seq_len x vocab_size => seq_len x vocab_size\n",
    "                                                        # for calculating loss function:\n",
    "                                                        # see here for details when implementing batching\n",
    "                                                        # https://discuss.pytorch.org/t/calculating-loss-for-entire-batch-using-nllloss-in-0-4-0/17142/7\n",
    "        \n",
    "        # targets should be caption without start index: i.e. [the blue one </s>] so we can predict\n",
    "        # next tokens from input like [<s> the blue one]\n",
    "\n",
    "        loss += criterion(model_output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "    def train_model(self):\n",
    "        if not self.initialized:\n",
    "            print(\"Make sure you initialize the model with the parameters you want\")\n",
    "            return\n",
    "\n",
    "\n",
    "        optimizer = self.optimizer(lr=self.lr, params=self.model.parameters())\n",
    "        criterion = self.criterion()\n",
    "\n",
    "        start_time = time.time()\n",
    "        store_losses_every = 100\n",
    "        print_losses_every = 1000\n",
    "        self.stored_losses = [] # theoretically we store losses so we can plot them later - \n",
    "                                # I don't think this part of the code works though. What we\n",
    "                                # can do instead is take a few thousand or so training examples\n",
    "                                # out and use them for \"evaluation\" every 1 epoch or so\n",
    "        for epoch in range(self.num_epochs):\n",
    "            print(\"---EPOCH {}---\".format(epoch))\n",
    "            stored_loss_total = 0\n",
    "            print_loss_total = 0\n",
    "\n",
    "            for i, pair in enumerate(self.features):\n",
    "                caption, colors = pair\n",
    "                caption = torch.tensor(caption, dtype=torch.long).view(-1, 1)\n",
    "                colors = torch.tensor(colors, dtype=torch.float)\n",
    "                target = self.targets[i] # already turned it into a tensor in `self.fit`\n",
    "\n",
    "\n",
    "                loss = self.train_iter(caption, colors, target, optimizer, criterion)\n",
    "                stored_loss_total += loss.item()\n",
    "                print_loss_total += loss.item()\n",
    "\n",
    "                if i % print_losses_every == 0:\n",
    "                    print_loss_avg = print_loss_total / print_losses_every\n",
    "                    print(\"{} ({}:{} {:.2f}%) {:.4f}\".format(self.asMinutes(time.time() - start_time),\n",
    "                                                      epoch, i, i/len(self.features)*100,\n",
    "                                                      print_loss_avg))\n",
    "                    print_loss_total = 0\n",
    "\n",
    "                if i % store_losses_every == 0:\n",
    "                    stored_loss_avg = stored_loss_total / store_losses_every\n",
    "                    self.stored_losses.append(stored_loss_avg)\n",
    "                    stored_loss_total = 0\n",
    "\n",
    "\n",
    "    def evaluate_iter(self, pair):\n",
    "        \"\"\"\n",
    "        Same as train_iter except don't use an optimizer and gradients or anything\n",
    "        like that\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            caption_tensor, color_tensor = pair\n",
    "            start_states = self.model.init_hidden_and_context()\n",
    "            model_output, _, _ = self.model(caption_tensor, start_states, color_tensor)\n",
    "\n",
    "            model_output = model_output.view(1, -1)\n",
    "            return model_output\n",
    "\n",
    "\n",
    "    def load_model(self, filename):\n",
    "        \"\"\"\n",
    "        Load model from saved file at filename\n",
    "        \"\"\"\n",
    "        if not self.initialized:\n",
    "            self.init_model\n",
    "        self.model.load_state_dict(torch.load(filename))\n",
    "\n",
    "    def save_model(self, filename):\n",
    "        \"\"\"\n",
    "        Save model to file at filename\n",
    "        \"\"\"\n",
    "        torch.save(self.model.state_dict(), filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.003003003003003"
      ]
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(-(np.log(0.333)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's evaluate on the dev set\n",
    "monroe_data_dev = MonroeData(\"../data/csv/dev_corpus_monroe.csv\", \"../data/entries/dev_entries_monroe.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_index = caption_phi.caption_indexer.get_idx_from_word(caption_phi.caption_indexer.EOS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, start_end_tokens = caption_phi.to_string_features(\"\")\n",
    "start_token = start_end_tokens[:1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_features_tensor = torch.tensor([np.flip(color_phi.to_color_features(monroe_data_dev[0].colors), axis=0).copy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    color_hidden = color_encoder(color_features_tensor)\n",
    "\n",
    "tokens = torch.tensor([start_token])\n",
    "with torch.no_grad():\n",
    "    for i in range(20):\n",
    "        vocab_preds = lit_speaker(color_hidden, tokens)[:,-1:,:]\n",
    "        _, prediction_index = vocab_preds.max(2)\n",
    "        tokens = torch.cat((tokens, prediction_index), dim=1)\n",
    "        if prediction_index.item() == end_index:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caption_phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0, 16,  5]])"
      ]
     },
     "execution_count": 387,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 3, 5])"
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens.view(-1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_indices_to_words(tokens):\n",
    "    tokens = tokens.view(-1)\n",
    "    words = []\n",
    "    for idx in tokens.numpy():\n",
    "        words.append(\n",
    "            caption_phi.caption_indexer.get_word_from_idx(idx)\n",
    "        )\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<sos>', 'brown', '<eos>']"
      ]
     },
     "execution_count": 388,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_indices_to_words(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0) Game: 2641-2 Round: 1\n",
      "gray\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWQAAADuCAYAAAAOR30qAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAA95JREFUeJzt27FNXEEUQNE/Fi0Qk5sCaATq2BDJ\nBVgi3DqgEQqAnBh6GAcUwP4Vq3+9nBO/kV50NRppxpxzAWB7v7ZeAIBPggwQIcgAEYIMECHIABGC\nDBAhyAARggwQIcgAERerpsfwre8MjK0XYL0xlsWv2v/Zx5zz8quhdUHmLDz+ud16BVa6+/u0/H55\n3noNjvR6ffN2yJwnC4CIo2/I+4fdd+7Biezu91uvABzIDRkgQpABIgQZIEKQASIEGSBCkAEiBBkg\nQpABIgQZIEKQASIEGSBCkAEiBBkgQpABIgQZIEKQASIEGSBCkAEiBBkgQpABIgQZIEKQASIEGSBC\nkAEiBBkgQpABIgQZIEKQASIEGSBCkAEiBBkgQpABIgQZIEKQASIEGSBCkAEiBBkgQpABIgQZIEKQ\nASIEGSBCkAEiBBkgQpABIgQZIEKQASIEGSBCkAEiBBkgQpABIgQZIEKQASIEGSBCkAEiBBkgQpAB\nIgQZIEKQASIEGSBCkAEiBBkgQpABIgQZIEKQASIEGSBCkAEiBBkgQpABIgQZIEKQASIEGSBCkAEi\nBBkgQpABIgQZIEKQASIEGSBCkAEiBBkgQpABIgQZIEKQASIEGSBCkAEiBBkgQpABIgQZIEKQASIE\nGSBCkAEiBBkgQpABIgQZIEKQASIEGSBCkAEiBBkgQpABIgQZIEKQASIEGSBCkAEiBBkgQpABIgQZ\nIEKQASIEGSBCkAEiBBkgQpABIgQZIEKQASIEGSBCkAEiBBkgQpABIgQZIEKQASIEGSBCkAEiBBkg\nQpABIgQZIEKQASIEGSBCkAEiBBkgQpABIgQZIEKQASIEGSBCkAEiBBkgQpABIgQZIEKQASIEGSBC\nkAEiBBkgQpABIgQZIEKQASIEGSBCkAEiBBkgQpABIgQZIEKQASIEGSBCkAEiBBkgQpABIgQZIEKQ\nASIEGSBCkAEiBBkgQpABIgQZIEKQASIEGSBCkAEiBBkgQpABIgQZIEKQASIEGSBCkAEiBBkgQpAB\nIgQZIEKQASIEGSBCkAEiBBkgQpABIgQZIEKQASIEGSBCkAEiBBkgQpABIgQZIEKQASIEGSBCkAEi\nBBkgQpABIgQZIEKQASIEGSBCkAEiBBkgQpABIgQZIEKQASIEGSBCkAEiBBkgQpABIgQZIEKQASIE\nGSBCkAEiBBkgQpABIgQZIEKQASIEGSBCkAEiBBkg4uLYg7v7/XfuAfDjuSEDRIw55+HDY7wvy/J2\nunUAztLVnPPyq6FVQQbgdDxZAEQIMkCEIANECDJAhCADRAgyQIQgA0QIMkCEIANE/AOZfSIxFrqi\n2AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "monroe_data_dev.display_game(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.0000e+00, -7.8835e-01,  2.4298e-01, -8.7009e-01,  3.8268e-01,\n",
       "           2.6671e-01,  5.1410e-01,  1.2241e-01, -7.0711e-01,  8.7009e-01,\n",
       "          -9.8918e-01,  6.8954e-01, -1.0000e+00,  7.8835e-01, -2.4298e-01,\n",
       "           8.7009e-01, -3.8268e-01, -2.6671e-01,  5.1410e-01, -9.3299e-01,\n",
       "           9.5694e-01, -8.7009e-01,  9.8918e-01, -6.8954e-01,  1.0000e+00,\n",
       "          -7.8835e-01,  2.4298e-01,  0.0000e+00, -6.1523e-01,  9.7003e-01,\n",
       "          -4.9290e-01,  9.2388e-01, -9.6378e-01,  8.5773e-01, -9.9248e-01,\n",
       "           7.0711e-01, -4.9290e-01, -1.4673e-01,  7.2425e-01, -1.2246e-16,\n",
       "           6.1523e-01, -9.7003e-01,  4.9290e-01, -9.2388e-01,  9.6378e-01,\n",
       "          -8.5773e-01,  3.5990e-01,  2.9028e-01,  4.9290e-01,  1.4673e-01,\n",
       "          -7.2425e-01,  0.0000e+00, -6.1523e-01,  9.7003e-01],\n",
       "         [ 1.0000e+00,  5.1410e-01, -4.7140e-01,  7.3565e-02, -8.1758e-01,\n",
       "          -9.1421e-01, -9.8918e-01, -6.3439e-01,  3.3689e-01, -5.1410e-01,\n",
       "          -1.0000e+00, -5.1410e-01, -8.9322e-01, -7.3565e-02,  8.1758e-01,\n",
       "           3.8268e-01,  9.8918e-01,  6.3439e-01, -4.7140e-01,  5.1410e-01,\n",
       "           1.0000e+00,  8.4485e-01,  8.9322e-01,  7.3565e-02,  5.9570e-01,\n",
       "          -3.8268e-01, -9.8918e-01,  0.0000e+00, -8.5773e-01, -8.8192e-01,\n",
       "          -9.9729e-01, -5.7581e-01,  4.0524e-01, -1.4673e-01,  7.7301e-01,\n",
       "           9.4154e-01, -8.5773e-01, -1.2246e-16,  8.5773e-01,  4.4961e-01,\n",
       "           9.9729e-01,  5.7581e-01,  9.2388e-01,  1.4673e-01, -7.7301e-01,\n",
       "           8.8192e-01,  8.5773e-01,  0.0000e+00,  5.3500e-01, -4.4961e-01,\n",
       "          -9.9729e-01, -8.0321e-01, -9.2388e-01, -1.4673e-01],\n",
       "         [ 1.0000e+00,  2.0711e-01, -9.1421e-01, -1.2241e-01, -9.9631e-01,\n",
       "          -2.9028e-01, -9.7003e-01,  3.6807e-02,  9.8528e-01, -2.0711e-01,\n",
       "          -1.0000e+00, -2.0711e-01, -9.4561e-01,  1.2241e-01,  9.9631e-01,\n",
       "           4.3862e-01,  9.7003e-01, -3.6807e-02, -9.1421e-01,  2.0711e-01,\n",
       "           1.0000e+00,  5.1410e-01,  9.4561e-01, -1.2241e-01,  7.8835e-01,\n",
       "          -4.3862e-01, -9.7003e-01,  0.0000e+00, -9.7832e-01, -4.0524e-01,\n",
       "          -9.9248e-01, -8.5797e-02,  9.5694e-01,  2.4298e-01,  9.9932e-01,\n",
       "           1.7096e-01, -9.7832e-01, -1.2246e-16,  9.7832e-01,  3.2531e-01,\n",
       "           9.9248e-01,  8.5797e-02,  8.9867e-01, -2.4298e-01, -9.9932e-01,\n",
       "           4.0524e-01,  9.7832e-01,  0.0000e+00,  8.5773e-01, -3.2531e-01,\n",
       "          -9.9248e-01, -6.1523e-01, -8.9867e-01,  2.4298e-01]]])"
      ]
     },
     "execution_count": 385,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "color_features_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.56640625, 0.5390625, 0.43359375]"
      ]
     },
     "execution_count": 371,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monroe_data_dev[0].colors[0].rgb_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.0000000e+00,  2.0711137e-01, -9.1420978e-01, -1.2241068e-01,\n",
       "       -9.9631262e-01, -2.9028466e-01, -9.7003126e-01,  3.6807224e-02,\n",
       "        9.8527765e-01, -2.0711137e-01, -1.0000000e+00, -2.0711137e-01,\n",
       "       -9.4560730e-01,  1.2241068e-01,  9.9631262e-01,  4.3861625e-01,\n",
       "        9.7003126e-01, -3.6807224e-02, -9.1420978e-01,  2.0711137e-01,\n",
       "        1.0000000e+00,  5.1410276e-01,  9.4560730e-01, -1.2241068e-01,\n",
       "        7.8834641e-01, -4.3861625e-01, -9.7003126e-01,  0.0000000e+00,\n",
       "       -9.7831738e-01, -4.0524131e-01, -9.9247956e-01, -8.5797310e-02,\n",
       "        9.5694035e-01,  2.4298018e-01,  9.9932235e-01,  1.7096189e-01,\n",
       "       -9.7831738e-01, -1.2246469e-16,  9.7831738e-01,  3.2531029e-01,\n",
       "        9.9247956e-01,  8.5797310e-02,  8.9867449e-01, -2.4298018e-01,\n",
       "       -9.9932235e-01,  4.0524131e-01,  9.7831738e-01,  0.0000000e+00,\n",
       "        8.5772860e-01, -3.2531029e-01, -9.9247956e-01, -6.1523157e-01,\n",
       "       -8.9867449e-01,  2.4298018e-01], dtype=float32)"
      ]
     },
     "execution_count": 373,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "color_phi_fourier(monroe_data_dev[0].colors[0].rgb_norm, \"rgb_norm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing new model module\n",
    "importlib.reload(models)\n",
    "import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [],
   "source": [
    "speaker_model_test = models.LiteralSpeaker(models.CaptionGenerator, optimizer=torch.optim.Adam, lr=0.004, num_epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_in_dim = 54\n",
    "color_dim = 100\n",
    "embed_dim = 100\n",
    "hidden_dim = 100\n",
    "#lit_speaker = Speaker(color_embed_dim, caption_phi.caption_indexer.size, embed_dim, hidden_dim)\n",
    "speaker_model_test.init_model(color_in_dim=color_in_dim, color_dim=color_dim, \n",
    "                              vocab_size=caption_phi.caption_indexer.size, embed_dim=embed_dim,\n",
    "                             speaker_hidden_dim=hidden_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(array([0, 1, 2, 3, 4, 5]),\n",
       "  array([[ 1.0000000e+00, -9.9247956e-01,  9.7003126e-01,  9.9247956e-01,\n",
       "          -1.0000000e+00,  9.9247956e-01,  9.7003126e-01, -9.9247956e-01,\n",
       "           1.0000000e+00,  8.7008697e-01, -9.2387950e-01,  9.6377605e-01,\n",
       "           8.0320752e-01, -8.7008697e-01,  9.2387950e-01,  7.2424710e-01,\n",
       "          -8.0320752e-01,  8.7008697e-01,  5.1410276e-01, -6.1523157e-01,\n",
       "           7.0710677e-01,  4.0524131e-01, -5.1410276e-01,  6.1523157e-01,\n",
       "           2.9028466e-01, -4.0524131e-01,  5.1410276e-01,  0.0000000e+00,\n",
       "          -1.2241068e-01,  2.4298018e-01, -1.2241068e-01, -1.2246469e-16,\n",
       "           1.2241068e-01, -2.4298018e-01,  1.2241068e-01,  0.0000000e+00,\n",
       "          -4.9289820e-01,  3.8268343e-01, -2.6671275e-01, -5.9569931e-01,\n",
       "           4.9289820e-01, -3.8268343e-01, -6.8954057e-01,  5.9569931e-01,\n",
       "          -4.9289820e-01, -8.5772860e-01,  7.8834641e-01, -7.0710677e-01,\n",
       "          -9.1420978e-01,  8.5772860e-01, -7.8834641e-01, -9.5694035e-01,\n",
       "           9.1420978e-01, -8.5772860e-01],\n",
       "         [ 1.0000000e+00, -9.7831738e-01,  9.1420978e-01,  9.7831738e-01,\n",
       "          -1.0000000e+00,  9.7831738e-01,  9.1420978e-01, -9.7831738e-01,\n",
       "           1.0000000e+00, -5.5557024e-01,  3.7131721e-01, -1.7096189e-01,\n",
       "          -7.1573085e-01,  5.5557024e-01, -3.7131721e-01, -8.4485358e-01,\n",
       "           7.1573085e-01, -5.5557024e-01, -3.8268343e-01,  5.6573182e-01,\n",
       "          -7.2424710e-01, -1.8303989e-01,  3.8268343e-01, -5.6573182e-01,\n",
       "           2.4541229e-02,  1.8303989e-01, -3.8268343e-01,  0.0000000e+00,\n",
       "          -2.0711137e-01,  4.0524131e-01, -2.0711137e-01, -1.2246469e-16,\n",
       "           2.0711137e-01, -4.0524131e-01,  2.0711137e-01,  0.0000000e+00,\n",
       "          -8.3146960e-01,  9.2850608e-01, -9.8527765e-01, -6.9837624e-01,\n",
       "           8.3146960e-01, -9.2850608e-01, -5.3499764e-01,  6.9837624e-01,\n",
       "          -8.3146960e-01,  9.2387950e-01, -8.2458931e-01,  6.8954057e-01,\n",
       "           9.8310548e-01, -9.2387950e-01,  8.2458931e-01,  9.9969882e-01,\n",
       "          -9.8310548e-01,  9.2387950e-01],\n",
       "         [ 1.0000000e+00, -9.5694035e-01,  8.3146960e-01,  6.2485951e-01,\n",
       "          -8.2458931e-01,  9.5330602e-01, -2.1910124e-01, -7.3564567e-02,\n",
       "           3.5989505e-01,  9.5694035e-01, -1.0000000e+00,  9.5694035e-01,\n",
       "           3.7131721e-01, -6.2485951e-01,  8.2458931e-01, -4.9289820e-01,\n",
       "           2.1910124e-01,  7.3564567e-02,  8.3146960e-01, -9.5694035e-01,\n",
       "           1.0000000e+00,  8.5797310e-02, -3.7131721e-01,  6.2485951e-01,\n",
       "          -7.2424710e-01,  4.9289820e-01, -2.1910124e-01,  0.0000000e+00,\n",
       "          -2.9028466e-01,  5.5557024e-01, -7.8073722e-01,  5.6573182e-01,\n",
       "          -3.0200595e-01, -9.7570211e-01,  9.9729043e-01, -9.3299282e-01,\n",
       "          -2.9028466e-01, -1.2246469e-16,  2.9028466e-01, -9.2850608e-01,\n",
       "           7.8073722e-01, -5.6573182e-01, -8.7008697e-01,  9.7570211e-01,\n",
       "          -9.9729043e-01, -5.5557024e-01,  2.9028466e-01,  0.0000000e+00,\n",
       "          -9.9631262e-01,  9.2850608e-01, -7.8073722e-01, -6.8954057e-01,\n",
       "           8.7008697e-01, -9.7570211e-01]], dtype=float32))]"
      ]
     },
     "execution_count": 418,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3, 4, 5])"
      ]
     },
     "execution_count": 434,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([targets[0]]).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---EPOCH 0---\n",
      "0m 0s (0:0 0.00%) 0.0069\n",
      "0m 5s (0:1000 6.38%) 2.9919\n",
      "0m 12s (0:2000 12.77%) 2.9471\n",
      "0m 19s (0:3000 19.15%) 3.1400\n",
      "0m 27s (0:4000 25.53%) 2.9943\n",
      "0m 34s (0:5000 31.92%) 3.2511\n",
      "0m 43s (0:6000 38.30%) 2.7954\n",
      "0m 52s (0:7000 44.69%) 2.4446\n",
      "1m 1s (0:8000 51.07%) 2.8511\n",
      "1m 9s (0:9000 57.45%) 2.2851\n",
      "1m 18s (0:10000 63.84%) 2.4198\n",
      "1m 27s (0:11000 70.22%) 2.1741\n",
      "1m 37s (0:12000 76.60%) 2.2271\n",
      "1m 46s (0:13000 82.99%) 2.4447\n",
      "1m 56s (0:14000 89.37%) 2.4467\n",
      "2m 5s (0:15000 95.75%) 2.7762\n",
      "---EPOCH 1---\n",
      "2m 12s (1:0 0.00%) 0.0013\n",
      "2m 22s (1:1000 6.38%) 2.2175\n",
      "2m 32s (1:2000 12.77%) 2.6495\n",
      "2m 42s (1:3000 19.15%) 2.8380\n",
      "2m 52s (1:4000 25.53%) 2.7904\n",
      "3m 2s (1:5000 31.92%) 3.1066\n",
      "3m 12s (1:6000 38.30%) 2.5828\n",
      "3m 22s (1:7000 44.69%) 2.2930\n",
      "3m 32s (1:8000 51.07%) 2.6774\n",
      "3m 42s (1:9000 57.45%) 2.1579\n",
      "3m 53s (1:10000 63.84%) 2.2542\n",
      "4m 4s (1:11000 70.22%) 2.0727\n",
      "4m 14s (1:12000 76.60%) 2.0752\n",
      "4m 25s (1:13000 82.99%) 2.3187\n",
      "4m 36s (1:14000 89.37%) 2.3312\n",
      "4m 47s (1:15000 95.75%) 2.6780\n",
      "---EPOCH 2---\n",
      "4m 54s (2:0 0.00%) 0.0013\n",
      "5m 5s (2:1000 6.38%) 2.1517\n",
      "5m 16s (2:2000 12.77%) 2.5945\n",
      "5m 28s (2:3000 19.15%) 2.7617\n",
      "5m 39s (2:4000 25.53%) 2.7621\n",
      "5m 50s (2:5000 31.92%) 3.0189\n",
      "6m 2s (2:6000 38.30%) 2.5458\n",
      "6m 14s (2:7000 44.69%) 2.2367\n",
      "6m 25s (2:8000 51.07%) 2.6171\n",
      "6m 36s (2:9000 57.45%) 2.1100\n",
      "6m 48s (2:10000 63.84%) 2.2196\n",
      "6m 59s (2:11000 70.22%) 2.0525\n",
      "7m 10s (2:12000 76.60%) 2.0558\n",
      "7m 22s (2:13000 82.99%) 2.2835\n",
      "7m 33s (2:14000 89.37%) 2.3395\n",
      "7m 45s (2:15000 95.75%) 2.6714\n",
      "---EPOCH 3---\n",
      "7m 53s (3:0 0.00%) 0.0013\n",
      "8m 5s (3:1000 6.38%) 2.1404\n",
      "8m 17s (3:2000 12.77%) 2.5426\n",
      "8m 29s (3:3000 19.15%) 2.7773\n",
      "8m 41s (3:4000 25.53%) 2.7270\n",
      "8m 53s (3:5000 31.92%) 2.9755\n",
      "9m 6s (3:6000 38.30%) 2.5229\n",
      "9m 18s (3:7000 44.69%) 2.2200\n",
      "9m 31s (3:8000 51.07%) 2.6080\n",
      "9m 43s (3:9000 57.45%) 2.1282\n",
      "9m 56s (3:10000 63.84%) 2.1984\n",
      "10m 8s (3:11000 70.22%) 2.0460\n",
      "10m 20s (3:12000 76.60%) 2.0329\n",
      "10m 33s (3:13000 82.99%) 2.2871\n",
      "10m 45s (3:14000 89.37%) 2.3065\n",
      "10m 58s (3:15000 95.75%) 2.6050\n",
      "---EPOCH 4---\n",
      "11m 7s (4:0 0.00%) 0.0016\n",
      "11m 19s (4:1000 6.38%) 2.1340\n",
      "11m 33s (4:2000 12.77%) 2.5211\n",
      "11m 46s (4:3000 19.15%) 2.7590\n",
      "12m 0s (4:4000 25.53%) 2.7047\n",
      "12m 13s (4:5000 31.92%) 2.9362\n",
      "12m 27s (4:6000 38.30%) 2.5234\n",
      "12m 40s (4:7000 44.69%) 2.2535\n",
      "12m 53s (4:8000 51.07%) 2.5822\n",
      "13m 7s (4:9000 57.45%) 2.1302\n",
      "13m 20s (4:10000 63.84%) 2.1850\n",
      "13m 33s (4:11000 70.22%) 2.0643\n",
      "13m 46s (4:12000 76.60%) 2.0306\n",
      "13m 59s (4:13000 82.99%) 2.2725\n",
      "14m 13s (4:14000 89.37%) 2.3104\n",
      "14m 28s (4:15000 95.75%) 2.6214\n"
     ]
    }
   ],
   "source": [
    "speaker_model_test.fit(features, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [],
   "source": [
    "speaker_model_test.save_model(\"../model/literal_speaker_5epoch.params\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([1, 2, 3, 4, 5]),\n",
       " array([6, 5]),\n",
       " array([ 7,  8,  9,  1,  7, 10,  4,  5]),\n",
       " array([11,  5]),\n",
       " array([115,  13,  14,   5])]"
      ]
     },
     "execution_count": 423,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
